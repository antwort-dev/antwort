# Brainstorm: Proactive Agent Scheduling

**Status**: Brainstorm (not yet a spec)
**Dependencies**: Spec 04 (Agentic Loop), Spec 05 (Storage), Spec 07 (Auth), Spec 12 (Function Registry)
**Inspired by**: OpenClaw cron/heartbeat system

## 1. Why Proactive Agents Matter

Most AI agent systems today are reactive: a user sends a message, the agent responds. The interaction begins and ends with the human. This is useful, but it leaves an entire class of work untouched: the work that needs to happen on a schedule, in response to external events, or as a chain reaction from other agent activity.

Proactive agents flip this around. They wake up on their own, perform tasks, and deliver results without anyone asking. Think of a security scanning agent that audits your infrastructure every morning at 6 AM, a weekly report generator that summarizes Jira activity every Friday at 5 PM, or a monitoring agent that triages PagerDuty alerts in real time. These agents work while you sleep.

OpenClaw proved the demand for this pattern. Their cron and heartbeat system is one of the features users mention most. But OpenClaw runs on the client side, which means the user's machine needs to be on and connected for scheduled work to happen.

Antwort runs on the server side. The gateway is always up. Kubernetes has native support for scheduled workloads (CronJobs). Multiple tenants can maintain independent schedules without interfering with each other. Server-side is the natural home for always-on proactive agents.

### Concrete Use Cases

| Use Case | Trigger | Agent Action | Output |
|----------|---------|-------------|--------|
| Daily security scan | Cron: `0 6 * * *` | Run OWASP checks against staging endpoints | Slack message with findings |
| Weekly Jira summary | Cron: `0 17 * * FRI` | Query Jira for completed stories, open blockers | Google Doc report |
| PR code review | Webhook: GitHub PR created | Read diff, analyze for bugs and style issues | GitHub PR comment |
| Alert triage | Webhook: PagerDuty alert fires | Investigate alert, check runbooks, suggest fix | Slack thread in #incidents |
| Data pipeline validation | Cron: `*/30 * * * *` | Query pipeline health endpoints, compare row counts | Dashboard update, alert on drift |
| Post-deploy smoke test | Completion hook: deploy agent finishes | Run integration tests against the new deployment | Slack notification with pass/fail |
| Cost anomaly detection | Cron: `0 8 * * MON` | Pull cloud billing data, compare week-over-week | Email report to finance team |

## 2. Trigger Types

### 2.1 Cron Triggers

Time-based schedules using standard cron syntax. The most straightforward trigger type.

```json
{
  "type": "cron",
  "expression": "0 9 * * MON-FRI",
  "timezone": "America/New_York"
}
```

Examples:
- `0 9 * * MON-FRI`: Weekday mornings at 9 AM
- `0 */4 * * *`: Every 4 hours
- `0 0 1 * *`: First day of every month at midnight
- `*/15 * * * *`: Every 15 minutes (careful with cost)

Timezone support is per-schedule. Without it, daylight saving transitions cause schedules to drift. The `timezone` field accepts IANA timezone names (`America/New_York`, `Europe/Berlin`, `UTC`).

### 2.2 Webhook Triggers

External systems fire agent invocations by sending HTTP requests to antwort. Antwort exposes per-schedule webhook endpoints.

```
POST /v1/webhooks/{trigger_id}
Content-Type: application/json
X-Hub-Signature-256: sha256=abc123...

{
  "action": "opened",
  "pull_request": {
    "number": 42,
    "title": "Add retry logic to payment service",
    "diff_url": "https://github.com/org/repo/pull/42.diff"
  }
}
```

The incoming webhook payload becomes available as a template variable in the agent's input. The agent receives the raw event data and decides what to do with it.

**Authentication for incoming webhooks:**

| Method | How it works | Best for |
|--------|-------------|----------|
| HMAC signature | Sender signs payload with shared secret, antwort verifies | GitHub, Stripe, most SaaS webhooks |
| Bearer token | Sender includes token in `Authorization` header | Internal services, custom integrations |
| Mutual TLS | Client certificate verification | High-security environments, service mesh |

```json
{
  "type": "webhook",
  "auth": {
    "type": "hmac",
    "algorithm": "sha256",
    "header": "X-Hub-Signature-256",
    "secret_ref": {
      "kind": "KubernetesSecret",
      "name": "github-webhook-secret",
      "namespace": "antwort",
      "key": "hmac-key"
    }
  }
}
```

Webhook secrets are never stored in the schedule object itself. They reference Kubernetes Secrets by name.

### 2.3 Completion Hooks

One agent invocation triggers another. Agent A completes, and its output becomes the input for Agent B. This enables simple pipeline workflows.

```json
{
  "type": "completion",
  "source_schedule_id": "sched_security_scan",
  "condition": "status == completed",
  "input_mapping": "previous.output"
}
```

Example pipeline:

```
Security Scan Agent (cron: daily 6 AM)
  └── on completion → Jira Ticket Agent (creates issues for findings)
        └── on completion → Slack Notification Agent (posts summary to #security)
```

The completion hook fires only when the source execution finishes with a matching condition. If the security scan fails, the Jira agent does not run (unless the condition explicitly matches failures).

Completion hooks create simple linear chains or fan-out patterns (one source triggers multiple downstream schedules). They do NOT support cycles (A triggers B triggers A). The scheduler detects and rejects cycles at creation time.

This is intentionally not a full workflow engine. If you need complex DAGs with conditional branching, retries per step, and parallel joins, use Argo Workflows or Tekton and trigger antwort via webhooks from there.

### 2.4 Event-Driven Triggers (Future)

Not in the initial implementation, but worth designing for:

- **Kubernetes Event watches**: trigger an agent when a Pod enters CrashLoopBackOff, when a Deployment rolls out, or when a PersistentVolumeClaim fills up
- **CloudEvents**: standardized event format from cloud providers (GCP Pub/Sub, AWS EventBridge, Azure Event Grid)
- **Message queue consumers**: Kafka, NATS, or Redis Streams

These would use a plugin interface (`TriggerSource`) that the cron, webhook, and completion triggers also implement internally. This keeps the door open without over-engineering the first release.

## 3. Schedule Data Model

```go
type Schedule struct {
    ID              string            `json:"id"`                // auto-generated, prefixed "sched_"
    Name            string            `json:"name"`              // human-readable label
    AgentID         string            `json:"agent_id"`          // reference to agent profile
    Trigger         TriggerConfig     `json:"trigger"`           // when this schedule fires
    InputTemplate   []InputItem       `json:"input_template"`    // what to send the agent
    DeliveryChannels []string         `json:"delivery_channels"` // where to send results
    Enabled         bool              `json:"enabled"`           // on/off switch
    MaxConcurrent   int               `json:"max_concurrent"`    // prevent runaway executions
    RetentionDays   int               `json:"retention_days"`    // how long to keep history
    LastRun         *time.Time        `json:"last_run"`          // last execution start
    NextRun         *time.Time        `json:"next_run"`          // next scheduled fire time
    CreatedAt       time.Time         `json:"created_at"`
    UpdatedAt       time.Time         `json:"updated_at"`
    CreatedBy       string            `json:"created_by"`        // tenant/user who created this
    Labels          map[string]string `json:"labels,omitempty"`  // arbitrary key-value metadata
}

type TriggerConfig struct {
    Type       string          `json:"type"`       // "cron", "webhook", "completion"
    Cron       *CronTrigger    `json:"cron,omitempty"`
    Webhook    *WebhookTrigger `json:"webhook,omitempty"`
    Completion *CompletionTrigger `json:"completion,omitempty"`
}

type CronTrigger struct {
    Expression string `json:"expression"` // standard 5-field cron
    Timezone   string `json:"timezone"`   // IANA timezone name
}

type WebhookTrigger struct {
    Auth WebhookAuth `json:"auth"`
}

type WebhookAuth struct {
    Type      string     `json:"type"`                 // "hmac", "bearer", "mtls"
    Algorithm string     `json:"algorithm,omitempty"`   // for hmac: "sha256", "sha1"
    Header    string     `json:"header,omitempty"`      // header containing signature
    SecretRef SecretRef  `json:"secret_ref,omitempty"`  // reference to K8s Secret
}

type SecretRef struct {
    Kind      string `json:"kind"`      // "KubernetesSecret"
    Name      string `json:"name"`
    Namespace string `json:"namespace"`
    Key       string `json:"key"`
}

type CompletionTrigger struct {
    SourceScheduleID string `json:"source_schedule_id"`
    Condition        string `json:"condition"` // "status == completed", "status == failed"
    InputMapping     string `json:"input_mapping"` // "previous.output"
}
```

### ID Generation

Schedule IDs follow the same pattern as other antwort resources: `sched_` prefix followed by a random suffix (e.g., `sched_a1b2c3d4e5`). Webhook trigger IDs are derived from the schedule ID: `whk_a1b2c3d4e5`. This makes the webhook endpoint deterministic and stable: `POST /v1/webhooks/whk_a1b2c3d4e5`.

## 4. Side-API: /v1/schedules

The schedules API is a management API that lives alongside the core Responses API. It follows the same conventions (JSON, standard error responses, auth middleware).

### Create a Schedule

```
POST /v1/schedules
Authorization: Bearer <token>
Content-Type: application/json

{
  "name": "Daily Security Scan",
  "agent_id": "agent_security_scanner",
  "trigger": {
    "type": "cron",
    "cron": {
      "expression": "0 6 * * *",
      "timezone": "America/New_York"
    }
  },
  "input_template": [
    {
      "type": "message",
      "role": "user",
      "content": "Run a security audit of the staging environment. Today is {{date}}. Focus on any changes since {{previous.completed_at}}."
    }
  ],
  "delivery_channels": ["chan_slack_security"],
  "enabled": true,
  "max_concurrent": 1,
  "retention_days": 90
}
```

Response:

```json
{
  "id": "sched_a1b2c3d4e5",
  "name": "Daily Security Scan",
  "agent_id": "agent_security_scanner",
  "trigger": {
    "type": "cron",
    "cron": {
      "expression": "0 6 * * *",
      "timezone": "America/New_York"
    }
  },
  "input_template": [
    {
      "type": "message",
      "role": "user",
      "content": "Run a security audit of the staging environment. Today is {{date}}. Focus on any changes since {{previous.completed_at}}."
    }
  ],
  "delivery_channels": ["chan_slack_security"],
  "enabled": true,
  "max_concurrent": 1,
  "retention_days": 90,
  "last_run": null,
  "next_run": "2026-02-23T06:00:00-05:00",
  "created_at": "2026-02-22T14:30:00Z",
  "updated_at": "2026-02-22T14:30:00Z",
  "created_by": "tenant_acme"
}
```

### Create a Webhook Schedule

```
POST /v1/schedules
Authorization: Bearer <token>
Content-Type: application/json

{
  "name": "PR Code Review",
  "agent_id": "agent_code_reviewer",
  "trigger": {
    "type": "webhook",
    "webhook": {
      "auth": {
        "type": "hmac",
        "algorithm": "sha256",
        "header": "X-Hub-Signature-256",
        "secret_ref": {
          "kind": "KubernetesSecret",
          "name": "github-webhook-secret",
          "namespace": "antwort",
          "key": "hmac-key"
        }
      }
    }
  },
  "input_template": [
    {
      "type": "message",
      "role": "user",
      "content": "Review this pull request:\nTitle: {{webhook.payload.pull_request.title}}\nDiff URL: {{webhook.payload.pull_request.diff_url}}\nDescription: {{webhook.payload.pull_request.body}}\n\nFocus on security issues, performance problems, and code style."
    }
  ],
  "delivery_channels": ["chan_github_pr_comment"],
  "enabled": true,
  "max_concurrent": 5,
  "retention_days": 30
}
```

Response includes the webhook endpoint URL:

```json
{
  "id": "sched_f6g7h8i9j0",
  "webhook_url": "https://antwort.example.com/v1/webhooks/whk_f6g7h8i9j0",
  "...": "..."
}
```

### Create a Completion Hook Schedule

```
POST /v1/schedules
Authorization: Bearer <token>
Content-Type: application/json

{
  "name": "File Jira Tickets for Security Findings",
  "agent_id": "agent_jira_filer",
  "trigger": {
    "type": "completion",
    "completion": {
      "source_schedule_id": "sched_a1b2c3d4e5",
      "condition": "status == completed",
      "input_mapping": "previous.output"
    }
  },
  "input_template": [
    {
      "type": "message",
      "role": "user",
      "content": "The security scan found the following issues:\n\n{{previous.output}}\n\nCreate a Jira ticket for each HIGH or CRITICAL finding in project SEC."
    }
  ],
  "delivery_channels": ["chan_slack_security"],
  "enabled": true,
  "max_concurrent": 1,
  "retention_days": 90
}
```

### List Schedules

```
GET /v1/schedules?enabled=true&trigger_type=cron&agent_id=agent_security_scanner
Authorization: Bearer <token>
```

Response:

```json
{
  "data": [
    {
      "id": "sched_a1b2c3d4e5",
      "name": "Daily Security Scan",
      "agent_id": "agent_security_scanner",
      "trigger": {"type": "cron"},
      "enabled": true,
      "last_run": "2026-02-22T06:00:00-05:00",
      "next_run": "2026-02-23T06:00:00-05:00"
    }
  ],
  "has_more": false
}
```

### Get Schedule Details

```
GET /v1/schedules/sched_a1b2c3d4e5
Authorization: Bearer <token>
```

Returns the full schedule object (same shape as the create response).

### Update a Schedule

```
PUT /v1/schedules/sched_a1b2c3d4e5
Authorization: Bearer <token>
Content-Type: application/json

{
  "trigger": {
    "type": "cron",
    "cron": {
      "expression": "0 7 * * *",
      "timezone": "America/New_York"
    }
  },
  "enabled": true
}
```

Partial updates (only the fields you send are changed). Updating a cron expression recalculates `next_run` immediately.

### Delete a Schedule

```
DELETE /v1/schedules/sched_a1b2c3d4e5
Authorization: Bearer <token>
```

Deleting a schedule also cancels any pending executions and removes the associated webhook endpoint (if it was a webhook trigger).

### Manual Trigger

```
POST /v1/schedules/sched_a1b2c3d4e5/trigger
Authorization: Bearer <token>
Content-Type: application/json

{
  "context": {
    "reason": "testing before production enable"
  }
}
```

Fires the schedule immediately, outside its normal cadence. Useful for testing a new schedule before enabling it. The execution is recorded in history with `trigger_source: "manual"`.

### List Executions

```
GET /v1/schedules/sched_a1b2c3d4e5/executions?status=failed&limit=10
Authorization: Bearer <token>
```

Response:

```json
{
  "data": [
    {
      "execution_id": "exec_x1y2z3",
      "schedule_id": "sched_a1b2c3d4e5",
      "trigger_source": "cron",
      "started_at": "2026-02-22T06:00:00-05:00",
      "completed_at": "2026-02-22T06:03:42-05:00",
      "status": "completed",
      "response_id": "resp_m4n5o6",
      "duration_seconds": 222,
      "error": null
    },
    {
      "execution_id": "exec_p7q8r9",
      "schedule_id": "sched_a1b2c3d4e5",
      "trigger_source": "cron",
      "started_at": "2026-02-21T06:00:00-05:00",
      "completed_at": "2026-02-21T06:00:15-05:00",
      "status": "failed",
      "response_id": null,
      "duration_seconds": 15,
      "error": "provider timeout: vllm did not respond within 30s"
    }
  ],
  "has_more": true
}
```

## 5. How Triggers Fire

### Cron Trigger Implementation

Two implementation approaches, each with distinct trade-offs:

**Option A: Internal Scheduler (recommended for initial release)**

The antwort process runs an internal goroutine that maintains a priority queue of schedule fire times. When the next fire time arrives, it creates and submits the execution.

```
┌─────────────────────────────────────────────┐
│ antwort process                             │
│                                             │
│  ┌───────────────┐   ┌──────────────────┐   │
│  │   Scheduler   │──>│  Engine Pipeline │   │
│  │  (goroutine)  │   │  (CreateResponse)│   │
│  │               │   └──────────────────┘   │
│  │  priority     │                          │
│  │  queue of     │   ┌──────────────────┐   │
│  │  next-fire    │──>│  Execution       │   │
│  │  times        │   │  History Store   │   │
│  └───────────────┘   └──────────────────┘   │
│                                             │
└─────────────────────────────────────────────┘
```

Pros:
- No external dependencies beyond what antwort already needs
- Simple deployment (no CronJob RBAC, no additional controllers)
- Sub-second scheduling precision
- Easy to test (inject a fake clock)

Cons:
- If antwort restarts, in-flight schedules might fire twice or be missed
- Only one replica can own the scheduler (requires leader election for HA)
- Memory usage grows with number of schedules (though a priority queue is compact)

**Option B: Kubernetes CronJobs**

Each cron schedule creates a corresponding Kubernetes CronJob that POSTs to antwort's internal trigger endpoint.

```
┌──────────────────┐     POST /internal/trigger/sched_xxx     ┌──────────────┐
│ K8s CronJob      │ ──────────────────────────────────────> │   antwort    │
│ (per schedule)   │                                          │   process    │
└──────────────────┘                                          └──────────────┘
```

Pros:
- Kubernetes handles scheduling, retries, and concurrency natively
- CronJob `concurrencyPolicy: Forbid` maps directly to `max_concurrent: 1`
- Survives antwort restarts (CronJob is a persistent resource)
- No leader election needed

Cons:
- Requires RBAC for CronJob/Job management
- Minimum resolution is 1 minute (CronJobs don't go below that)
- One CronJob per schedule adds Kubernetes API load
- More complex deployment (need ServiceAccount with Job permissions)
- Harder to test locally

**Recommendation**: Start with the internal scheduler (Option A). It covers the common case, is simpler to deploy, and gives sub-minute scheduling for high-frequency triggers. Add CronJob support as an alternative backend for teams that want Kubernetes-native HA without leader election.

### Execution Pipeline

When any trigger fires (cron tick, webhook received, upstream completion), the scheduler follows this sequence:

```
1. Load Schedule
   └── Read schedule config from storage
   └── Check: is the schedule enabled?
   └── Check: is max_concurrent exceeded?

2. Load Agent Profile
   └── Resolve agent_id to agent configuration
   └── Get model, system prompt, tool definitions, parameters

3. Render Input Template
   └── Replace {{date}} with current date
   └── Replace {{now}} with current ISO timestamp
   └── Replace {{webhook.payload}} with incoming webhook body (if webhook trigger)
   └── Replace {{previous.output}} with last execution's output (if completion hook)
   └── Replace {{schedule.name}} with schedule name

4. Create Execution Record
   └── Generate execution_id
   └── Record started_at, trigger_source, status: "running"

5. Build CreateResponseRequest
   └── Standard Responses API request with rendered input
   └── Agent's model, tools, system prompt, parameters
   └── store: true (so the response is persisted)

6. Execute Through Engine
   └── Normal engine pipeline: provider call, agentic loop, tool execution
   └── Same code path as a user-initiated /v1/responses request
   └── No special scheduling-aware code in the engine

7. Deliver Results
   └── Look up delivery_channels
   └── Push results to each channel (Slack, email, webhook, etc.)
   └── If no channels configured, response is simply stored

8. Update Execution Record
   └── Record completed_at, status, response_id
   └── Update schedule's last_run, calculate next_run
   └── If completion hooks exist for this schedule, fire them
```

The key principle: step 6 is completely unaware that this is a scheduled execution. The engine sees a normal `CreateResponseRequest`. This means scheduled agents get the same tool access, sandbox isolation, streaming behavior, and error handling as user-initiated requests.

## 6. Execution History

Every trigger firing creates an execution record.

```go
type Execution struct {
    ExecutionID     string     `json:"execution_id"`      // "exec_" prefix
    ScheduleID      string     `json:"schedule_id"`
    TriggerSource   string     `json:"trigger_source"`    // "cron", "webhook", "completion", "manual"
    StartedAt       time.Time  `json:"started_at"`
    CompletedAt     *time.Time `json:"completed_at"`
    Status          string     `json:"status"`            // "running", "completed", "failed", "skipped"
    ResponseID      *string    `json:"response_id"`       // link to Responses API response
    DurationSeconds *float64   `json:"duration_seconds"`
    Error           *string    `json:"error"`
    WebhookPayload  *json.RawMessage `json:"webhook_payload,omitempty"` // stored for debugging
    TriggerContext  map[string]string `json:"trigger_context,omitempty"` // additional metadata
}
```

### Status Values

| Status | Meaning |
|--------|---------|
| `running` | Execution is in progress |
| `completed` | Agent finished successfully, results delivered |
| `failed` | Execution failed (provider error, tool error, timeout) |
| `skipped` | Trigger fired but execution was skipped (max_concurrent exceeded, schedule disabled) |

### Retention

Execution history is retained according to the schedule's `retention_days` setting. A background cleanup job runs periodically to remove expired records. The associated response in the Responses API is NOT deleted (it has its own retention policy). Only the execution metadata is cleaned up.

Default retention: 30 days. Maximum: 365 days. Configurable per schedule.

### PostgreSQL Schema (sketch)

```sql
CREATE TABLE schedule_executions (
    execution_id    TEXT PRIMARY KEY,
    schedule_id     TEXT NOT NULL REFERENCES schedules(id) ON DELETE CASCADE,
    trigger_source  TEXT NOT NULL,
    started_at      TIMESTAMPTZ NOT NULL DEFAULT now(),
    completed_at    TIMESTAMPTZ,
    status          TEXT NOT NULL DEFAULT 'running',
    response_id     TEXT,
    duration_seconds DOUBLE PRECISION,
    error           TEXT,
    webhook_payload JSONB,
    trigger_context JSONB
);

CREATE INDEX idx_executions_schedule_id ON schedule_executions(schedule_id);
CREATE INDEX idx_executions_status ON schedule_executions(status);
CREATE INDEX idx_executions_started_at ON schedule_executions(started_at);
```

## 7. Concurrency and Rate Limiting

### Max Concurrent Executions

Each schedule has a `max_concurrent` field. Before starting a new execution, the scheduler checks how many executions for this schedule are currently in `running` status.

- If `count(running) >= max_concurrent`, the trigger is skipped and logged with status `skipped`.
- For cron schedules, this typically means the previous execution is still running when the next tick arrives. A `max_concurrent: 1` setting means "don't overlap."
- For webhook schedules, higher concurrency is common. A PR review agent might handle 5 concurrent reviews (`max_concurrent: 5`).

### Queue Depth Limit

If a webhook schedule receives a burst of 100 requests in 10 seconds, we should not try to execute all of them simultaneously even if `max_concurrent` allows it. A per-schedule queue depth limit (default: 50) prevents memory exhaustion from piled-up pending executions. Excess triggers are rejected with HTTP 429.

### Per-Tenant Limits

Global limits prevent one tenant from consuming all scheduling capacity:

| Limit | Default | Configurable |
|-------|---------|-------------|
| Max schedules per tenant | 100 | Yes |
| Max concurrent executions per tenant | 20 | Yes |
| Max webhook triggers per minute per tenant | 60 | Yes |
| Max cron frequency per schedule | `*/1 * * * *` (every minute) | Yes |

### Backpressure from Sandbox Pool

If the sandbox warm pool is exhausted (all pre-warmed pods are in use), scheduled executions that require sandbox tools will be delayed. The scheduler does not fail them immediately. Instead, it retries with exponential backoff up to a configurable timeout (default: 5 minutes). If the sandbox pool remains full after the timeout, the execution fails with an error.

## 8. Delivery Integration

Scheduled executions produce results, but no client is waiting for the HTTP response. The results need to go somewhere.

### Default Behavior

Every execution stores its response in the database via the normal Responses API. The `response_id` in the execution record links to the stored response. You can always retrieve results via `GET /v1/responses/{response_id}`.

### Delivery Channels (cross-reference: brainstorm 05)

For push-based delivery, schedules reference delivery channels by ID. Channel configuration is its own API (`/v1/channels`), covered in a separate brainstorm. Example channel types:

| Channel Type | What it does |
|-------------|-------------|
| `slack` | Posts agent output to a Slack channel or thread |
| `email` | Sends agent output as an email |
| `webhook_out` | POSTs agent output to an external URL |
| `github_pr` | Comments on a GitHub pull request |
| `google_doc` | Appends to or creates a Google Doc |

A schedule can reference multiple delivery channels. After the agent completes, the delivery system sends the output to each channel in parallel. Delivery failures are logged but do not mark the execution as failed (the agent work is done, delivery is best-effort with retries).

### Delivery Payload

What gets sent to each channel is the agent's final text output by default. For structured delivery (e.g., posting JSON to a webhook), the agent profile can define an output schema, and the delivery system formats accordingly.

## 9. Template Variables

Input templates support variable substitution. Variables are enclosed in double curly braces and resolved at execution time, before the request enters the engine.

### Available Variables

| Variable | Type | Available In | Description |
|----------|------|-------------|-------------|
| `{{now}}` | string | All triggers | Current ISO 8601 timestamp |
| `{{date}}` | string | All triggers | Current date (YYYY-MM-DD) |
| `{{time}}` | string | All triggers | Current time (HH:MM:SS) |
| `{{day_of_week}}` | string | All triggers | Monday, Tuesday, etc. |
| `{{schedule.id}}` | string | All triggers | Schedule ID |
| `{{schedule.name}}` | string | All triggers | Schedule display name |
| `{{execution.id}}` | string | All triggers | Current execution ID |
| `{{webhook.payload}}` | JSON | Webhook triggers | Full incoming webhook body |
| `{{webhook.payload.X.Y}}` | any | Webhook triggers | Dot-path into webhook payload |
| `{{webhook.headers.X}}` | string | Webhook triggers | Incoming request header value |
| `{{previous.output}}` | string | Completion hooks | Text output from source execution |
| `{{previous.response_id}}` | string | Completion hooks | Response ID from source execution |
| `{{previous.completed_at}}` | string | All (if prior run exists) | Timestamp of last successful execution |
| `{{trigger.context.X}}` | any | Manual triggers | Custom context passed via manual trigger |

### Template Rendering

Templates use a simple `{{variable}}` syntax, not a full template engine like Go's `text/template`. No conditionals, no loops, no function calls. Just variable substitution.

If a variable is not available (e.g., `{{webhook.payload}}` in a cron trigger), it renders as an empty string. This is intentional: it allows shared templates that work across trigger types.

Rendering happens in the scheduler, before the request is passed to the engine. The engine receives a fully rendered `CreateResponseRequest` with no template syntax.

### Example: Template with Webhook Data

```json
{
  "input_template": [
    {
      "type": "message",
      "role": "user",
      "content": "A new PR was opened in {{webhook.payload.repository.full_name}}.\n\nTitle: {{webhook.payload.pull_request.title}}\nAuthor: {{webhook.payload.pull_request.user.login}}\nDiff: {{webhook.payload.pull_request.diff_url}}\n\nReview this PR for:\n1. Security vulnerabilities\n2. Performance issues\n3. Code style violations\n\nProvide your review as a GitHub PR comment."
    }
  ]
}
```

## 10. Security Considerations

### Authorization: Who Can Create Schedules?

Schedules execute agent invocations on behalf of a tenant. Creating a schedule is equivalent to saying "run this agent repeatedly, charged to my account." This needs careful access control.

| Action | Required Role |
|--------|--------------|
| Create schedule | `schedule:write` (typically platform-admin or schedule-admin) |
| List/get schedules | `schedule:read` (any authenticated user in the tenant) |
| Update schedule | `schedule:write` + must be in same tenant as schedule creator |
| Delete schedule | `schedule:write` + must be in same tenant as schedule creator |
| Manual trigger | `schedule:execute` |
| View executions | `schedule:read` |

### Tenant Isolation

Scheduled executions run in the schedule creator's tenant context. The tenant ID is recorded at schedule creation time and cannot be changed. All storage queries, tool access, and delivery channels are scoped to that tenant.

If a tenant is deactivated, all their schedules are automatically disabled.

### Webhook Security

Webhook endpoints are publicly reachable (that's the point: external services need to call them). This makes them an attack surface.

Defenses:
1. **Authentication**: Every webhook trigger requires an auth configuration. Unauthenticated webhooks are not supported.
2. **Rate limiting**: Per-webhook rate limits (default: 60/min) prevent abuse. Excess requests get HTTP 429.
3. **Payload size limit**: Maximum webhook payload size (default: 1 MB) prevents memory attacks.
4. **IP allowlisting** (optional): For known webhook sources (GitHub's published IP ranges), allow only those IPs.
5. **Replay prevention**: Include a timestamp in the HMAC signature and reject requests older than 5 minutes.

### Audit Logging

Every schedule operation is audit-logged:
- Schedule created/updated/deleted (who, when, what changed)
- Execution started (trigger source, schedule ID, tenant)
- Execution completed/failed (duration, status, error)
- Webhook received (source IP, trigger ID, auth result)

Audit logs are structured (JSON) and emitted via `slog`. They can be collected by any standard log aggregation system.

## 11. Monitoring and Alerting

### Prometheus Metrics

```
# Schedule execution counters
antwort_schedule_executions_total{schedule_id, schedule_name, trigger_type, status}
antwort_schedule_executions_skipped_total{schedule_id, schedule_name, reason}

# Execution duration
antwort_schedule_execution_duration_seconds{schedule_id, schedule_name, trigger_type}  # histogram

# Next fire time (useful for detecting stuck schedulers)
antwort_schedule_next_fire_seconds{schedule_id, schedule_name}  # gauge: seconds until next execution

# Webhook-specific
antwort_webhook_requests_total{trigger_id, status_code}
antwort_webhook_auth_failures_total{trigger_id}
antwort_webhook_payload_bytes{trigger_id}  # histogram

# Scheduler health
antwort_scheduler_schedules_active{trigger_type}  # gauge
antwort_scheduler_queue_depth{}  # gauge: pending executions waiting to start
antwort_scheduler_leader{instance}  # gauge: 1 if this instance is the scheduler leader
```

### Alerting Rules (examples)

```yaml
# Alert if a schedule has failed 3 times in a row
- alert: ScheduleRepeatedFailure
  expr: |
    increase(antwort_schedule_executions_total{status="failed"}[1h]) >= 3
    and
    increase(antwort_schedule_executions_total{status="completed"}[1h]) == 0
  for: 5m
  labels:
    severity: warning
  annotations:
    summary: "Schedule {{ $labels.schedule_name }} has failed 3+ times without success"

# Alert if the scheduler hasn't fired when expected
- alert: ScheduleMissedExecution
  expr: antwort_schedule_next_fire_seconds < -300
  for: 5m
  labels:
    severity: critical
  annotations:
    summary: "Schedule {{ $labels.schedule_name }} is overdue by 5+ minutes"

# Alert on webhook auth failures (possible attack)
- alert: WebhookAuthFailureSpike
  expr: rate(antwort_webhook_auth_failures_total[5m]) > 10
  for: 2m
  labels:
    severity: warning
  annotations:
    summary: "High webhook auth failure rate on trigger {{ $labels.trigger_id }}"
```

### Health Check

The `/healthz` endpoint should report scheduler status:

```json
{
  "status": "healthy",
  "scheduler": {
    "is_leader": true,
    "active_schedules": 42,
    "running_executions": 3,
    "next_fire_in_seconds": 127
  }
}
```

If the scheduler is not the leader (in a multi-replica deployment), it reports `is_leader: false` and does not fire triggers.

## 12. Leader Election (HA)

In a multi-replica deployment, only one antwort instance should own the internal scheduler. The others stand by as followers.

Options for leader election:

| Method | Pros | Cons |
|--------|------|------|
| Kubernetes Lease | Native K8s, no external dependency | Requires RBAC for Lease objects |
| PostgreSQL advisory lock | Already have a database connection | Adds load to the database |
| etcd lease (if available) | Battle-tested, used by K8s itself | Additional dependency |

**Recommendation**: Kubernetes Lease. It's the standard approach for Go controllers, the `client-go` library has built-in support (`leaderelection` package), and antwort already runs in Kubernetes.

On leader loss:
1. Current leader stops the scheduler goroutine
2. New leader loads all enabled schedules from the database
3. New leader recalculates next-fire times from `last_run` timestamps
4. Schedules that were missed during the transition fire immediately (configurable: `fire_on_miss: true`)

## 13. Open Questions

### Should schedules support retry on failure?

A failed execution could be retried automatically. The question is how sophisticated the retry logic should be.

Minimal approach: `retry_count: 3` and `retry_delay: "30s"` fields on the schedule. Fixed delay, no exponential backoff. Retries appear as separate execution records linked to the original.

More complex: exponential backoff, configurable retry conditions (retry on provider timeout but not on tool error), dead-letter behavior after max retries. This starts to look like a job queue.

**Leaning toward**: Minimal approach. Simple retries cover the most common case (transient provider errors). For complex retry logic, use an external system and trigger antwort via webhooks.

### How complex should completion hooks be?

Current proposal: simple linear chains and fan-out. One schedule can trigger multiple downstream schedules on completion. No cycles, no conditional branching, no parallel joins.

Possible extensions:
- Conditional routing: "if security scan found critical issues, trigger incident response agent; otherwise, trigger summary agent"
- Parallel join: "wait for both agent A and agent B to complete, then trigger agent C"
- Dynamic fan-out: "for each finding in the security scan output, trigger a separate investigation agent"

**Leaning toward**: Keep it simple for v1. Conditional routing is the most likely extension. The others are workflow engine territory.

### Should webhook triggers support payload filtering?

Example: a GitHub webhook fires for all PR events (opened, closed, merged, commented). Should the schedule be able to filter to only `action == "opened"`?

```json
{
  "type": "webhook",
  "webhook": {
    "auth": {"...": "..."},
    "filter": {
      "jq_expression": ".action == \"opened\" and .pull_request.base.ref == \"main\""
    }
  }
}
```

This is useful but adds complexity. Without filtering, the agent receives all events and decides what to do (which is fine for smart agents). Filtering saves compute by skipping irrelevant events before the agent runs.

**Leaning toward**: Add simple field-matching filters (not full jq). Something like:

```json
"filter": {
  "match": {
    "action": "opened",
    "pull_request.base.ref": "main"
  }
}
```

### Integration with Kubernetes Events

Should antwort watch Kubernetes Events as a trigger source? Examples:
- Pod enters CrashLoopBackOff, agent investigates
- Deployment rollout completes, agent runs smoke tests
- PersistentVolumeClaim reaches 90% capacity, agent alerts

This is powerful but raises scope questions. Antwort would need a Kubernetes watch/informer, RBAC to read events, and filtering logic. It might be better suited as an external event forwarder that translates K8s Events into antwort webhook calls.

**Leaning toward**: External forwarder. A small sidecar or separate Deployment watches K8s events and POSTs relevant ones to antwort's webhook endpoints. Keeps antwort's RBAC footprint small and the scheduling system generic.

### Timezone Handling Edge Cases

What happens when a cron schedule is set for 2:30 AM in a timezone that observes daylight saving time?
- Spring forward: 2:30 AM doesn't exist. Fire at 3:00 AM (next valid time) or skip?
- Fall back: 2:30 AM happens twice. Fire once or twice?

Most cron implementations fire once and skip the non-existent time. We should document this behavior explicitly and match what users expect from standard cron.

### Schedule Versioning

When a schedule is updated, should we keep a history of previous configurations? This would allow answering questions like "this schedule used to run at 6 AM but someone changed it to 7 AM, when did that happen?"

**Leaning toward**: No explicit versioning. The audit log captures changes. Adding a full version history for schedule configs adds storage complexity for marginal benefit.
