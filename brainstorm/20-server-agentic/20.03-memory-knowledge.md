# Brainstorm: Memory and Knowledge Management

**Dependencies**: Spec 005 (Storage), Brainstorm 12 (Function Registry), Brainstorm 14 (File Search Provider)
**Packages**: `pkg/memory/`, `pkg/tools/builtins/filesearch/`, extensions to `pkg/storage/`

## Purpose

Antwort already persists conversation state via `previous_response_id` chaining and PostgreSQL storage. This brainstorm explores extending that foundation into a full memory and knowledge system: vector-based retrieval (RAG), conversation summarization, and cross-conversation episodic memory.

The goal is to give agents persistent context that survives individual conversations, and to let users upload documents that agents can search at runtime.

## Three Types of Memory

```
                     Conversation Memory
                    (already implemented)
                            |
         +------------------+------------------+
         |                                     |
    Episodic Memory                    Semantic Memory
 (extracted key facts)                (uploaded documents)
         |                                     |
  Stored as structured              Stored as vector
  memory entries                    embeddings (RAG)
         |                                     |
  Retrieved by relevance            Retrieved by file_search
  to current conversation          tool during agentic loop
```

### 1. Conversation Memory (exists today)

What happened in the current conversation. Already implemented via `previous_response_id` chaining in `pkg/storage/` and `pkg/engine/history.go`.

The `loadConversationHistory` function follows the response chain using `GetResponseForChain`, collects all input/output items, and feeds them back to the provider as messages. This is the simplest form of memory, and it works well for short conversations.

**Current limitation**: For long conversations, the full chain may exceed the LLM's context window. The chain-walking approach in `history.go` has no truncation or summarization, it just loads everything.

### 2. Episodic Memory (new)

Key facts and decisions extracted from past conversations, stored as structured entries that can be recalled in future conversations. Think of it as "what did this user discuss before?" across conversation boundaries.

Examples of episodic memories:
- "User prefers Python over Go for scripting tasks"
- "User's project uses PostgreSQL 16 with pgvector"
- "User decided to use Kustomize instead of Helm for deployment"
- "Previous conversation concluded that the auth middleware needs rate limiting"

### 3. Semantic Memory / Knowledge (new, RAG)

Documents uploaded by users, chunked, embedded, and indexed in a vector store. At runtime, the `file_search` tool queries these vectors and injects relevant chunks into the LLM context. This is standard RAG.

## Knowledge: Vector Stores for RAG

### The file_search Tool Type

The Responses API supports a `file_search` tool that searches uploaded documents:

```json
{
  "model": "meta-llama/Llama-3.1-70B-Instruct",
  "tools": [
    {
      "type": "file_search",
      "vector_store_ids": ["vs_abc123"]
    }
  ],
  "input": [
    {
      "type": "message",
      "role": "user",
      "content": [
        {
          "type": "input_text",
          "text": "What does our architecture document say about auth?"
        }
      ]
    }
  ]
}
```

When the model decides to search, it calls `file_search` with a query string. The engine intercepts this call (just like any other server-executed tool), queries the vector store, and returns relevant document chunks as the tool result. The model then uses those chunks to formulate its answer.

### Side-API: Vector Store Management

Vector stores are managed through a set of OpenAI-compatible REST endpoints. These are registered as management API routes by the `FileSearchProvider` through the Function Registry (Brainstorm 12).

#### Create a Vector Store

```
POST /v1/vector_stores
Content-Type: application/json
Authorization: Bearer <token>

{
  "name": "project-docs",
  "metadata": {
    "project": "antwort",
    "team": "platform"
  }
}
```

Response:

```json
{
  "id": "vs_abc123",
  "object": "vector_store",
  "name": "project-docs",
  "status": "completed",
  "file_counts": {
    "in_progress": 0,
    "completed": 0,
    "failed": 0,
    "total": 0
  },
  "metadata": {
    "project": "antwort",
    "team": "platform"
  },
  "created_at": 1709155200,
  "expires_at": null
}
```

#### Upload a Document

```
POST /v1/vector_stores/vs_abc123/files
Content-Type: multipart/form-data
Authorization: Bearer <token>

file: @architecture.pdf
chunking_strategy: {"type": "auto"}
```

Response:

```json
{
  "id": "vsf_file789",
  "object": "vector_store.file",
  "vector_store_id": "vs_abc123",
  "status": "in_progress",
  "filename": "architecture.pdf",
  "bytes": 245760,
  "chunking_strategy": {
    "type": "auto",
    "chunk_size": 512,
    "chunk_overlap": 50
  },
  "created_at": 1709155300
}
```

The upload returns immediately with `status: "in_progress"`. Document parsing, chunking, and embedding happen asynchronously. The client can poll the file status to check when indexing completes.

#### List Documents in a Store

```
GET /v1/vector_stores/vs_abc123/files
Authorization: Bearer <token>
```

Response:

```json
{
  "object": "list",
  "data": [
    {
      "id": "vsf_file789",
      "object": "vector_store.file",
      "vector_store_id": "vs_abc123",
      "status": "completed",
      "filename": "architecture.pdf",
      "bytes": 245760,
      "chunk_count": 47,
      "created_at": 1709155300
    }
  ]
}
```

#### Manual Search (for debugging)

```
POST /v1/vector_stores/vs_abc123/search
Content-Type: application/json
Authorization: Bearer <token>

{
  "query": "authentication middleware design",
  "max_results": 5
}
```

Response:

```json
{
  "object": "list",
  "data": [
    {
      "score": 0.89,
      "content": "The auth middleware extracts the tenant ID from the JWT claims and injects it into the request context via storage.SetTenant()...",
      "filename": "architecture.pdf",
      "file_id": "vsf_file789",
      "chunk_index": 12
    },
    {
      "score": 0.82,
      "content": "API key authentication is implemented in pkg/auth/apikey/apikey.go. The middleware validates the key against a configured list...",
      "filename": "architecture.pdf",
      "file_id": "vsf_file789",
      "chunk_index": 14
    }
  ]
}
```

#### Full API Surface

| Method | Path | Description |
|--------|------|-------------|
| `POST` | `/v1/vector_stores` | Create a vector store |
| `GET` | `/v1/vector_stores` | List vector stores |
| `GET` | `/v1/vector_stores/{id}` | Get vector store details |
| `DELETE` | `/v1/vector_stores/{id}` | Delete vector store and all its files |
| `POST` | `/v1/vector_stores/{id}/files` | Upload and index a document |
| `GET` | `/v1/vector_stores/{id}/files` | List documents in store |
| `GET` | `/v1/vector_stores/{id}/files/{file_id}` | Get document status |
| `DELETE` | `/v1/vector_stores/{id}/files/{file_id}` | Remove a document |
| `POST` | `/v1/vector_stores/{id}/search` | Manual search (debug/test) |

### Document Processing Pipeline

```
┌──────────┐    ┌─────────┐    ┌─────────┐    ┌─────────┐    ┌──────────┐
│  Upload  │───>│  Parse  │───>│  Chunk  │───>│  Embed  │───>│  Store   │
│  (file)  │    │ (text)  │    │ (parts) │    │ (vecs)  │    │ (pgvec)  │
└──────────┘    └─────────┘    └─────────┘    └─────────┘    └──────────┘
     │                                                            │
     │          Runs asynchronously after upload returns           │
     │                                                            │
     └── status: "in_progress" ──────────────────────────────────>│
                                                  status: "completed"
```

**Parse**: Extract text from the uploaded file. Supported formats:
- Plain text (`.txt`, `.md`, `.csv`): direct read
- PDF: text extraction via a Go PDF library
- DOCX: XML extraction from the ZIP container (Go stdlib `archive/zip`)
- HTML: `golang.org/x/net/html` for tag stripping

**Chunk**: Split extracted text into segments suitable for embedding and retrieval.

| Strategy | Description | When to use |
|----------|-------------|-------------|
| Fixed-size | Split every N tokens with overlap | Default, works well for most content |
| Recursive | Split on paragraph, sentence, then word boundaries | Better semantic coherence |
| Semantic | Use embedding similarity to find natural break points | Best quality, slowest |

Default: Fixed-size with 512 tokens per chunk and 50 tokens overlap. Configurable per vector store or per upload.

**Embed**: Convert each chunk into a vector using an embedding model. The embedding endpoint is configurable. If the LLM backend (vLLM, LiteLLM) supports `/v1/embeddings`, use that. Otherwise, fall back to a dedicated embedding service URL.

**Store**: Write the vector and metadata into pgvector (or an alternative vector backend).

### Chunking Data Model

```go
// Chunk represents a single chunk of a document, ready for embedding.
type Chunk struct {
    ID            string  // e.g., "chunk_abc123"
    FileID        string  // parent file: "vsf_file789"
    VectorStoreID string  // parent store: "vs_abc123"
    TenantID      string  // tenant isolation
    Content       string  // the text content of this chunk
    ChunkIndex    int     // position within the document (0-based)
    TokenCount    int     // token count of this chunk
    Embedding     []float32 // the embedding vector
}
```

### Runtime: file_search in the Agentic Loop

When the model produces a `file_search` tool call during the agentic loop:

```
1. Engine receives function_call item: name="file_search"
2. FunctionRegistry routes to FileSearchProvider.Execute()
3. FileSearchProvider:
   a. Parses the query from the tool call arguments
   b. Generates an embedding for the query text
   c. Runs a vector similarity search against the configured vector stores
   d. Ranks results by cosine similarity score
   e. Returns the top-K chunks as the tool result
4. Engine feeds the result back as a function_call_output item
5. Model uses the retrieved chunks to formulate its answer
```

The tool result is formatted as structured text the model can reference:

```json
{
  "results": [
    {
      "file_id": "vsf_file789",
      "filename": "architecture.pdf",
      "score": 0.89,
      "content": "The auth middleware extracts the tenant ID..."
    }
  ]
}
```

### Annotation Support

When the model cites content from file_search results, the response can include annotations:

```json
{
  "type": "output_text",
  "text": "According to the architecture document, the auth middleware extracts tenant IDs from JWT claims [1].",
  "annotations": [
    {
      "type": "file_citation",
      "text": "[1]",
      "start_index": 89,
      "end_index": 92,
      "file_id": "vsf_file789",
      "filename": "architecture.pdf"
    }
  ]
}
```

This uses the existing `Annotation` type from `pkg/api/types.go`. The `file_citation` annotation type is a new annotation kind alongside the existing structure.

## Episodic Memory

### Memory Entry Model

```go
// MemoryEntry represents a single fact or insight extracted from a conversation.
type MemoryEntry struct {
    ID                   string            `json:"id"`                     // e.g., "mem_abc123"
    TenantID             string            `json:"tenant_id"`             // tenant isolation
    Fact                 string            `json:"fact"`                  // the extracted fact
    SourceConversationID string            `json:"source_conversation_id"` // response ID where this was extracted
    Timestamp            time.Time         `json:"timestamp"`
    Confidence           float64           `json:"confidence"`            // 0.0 to 1.0
    Tags                 []string          `json:"tags"`                  // categorization
    Embedding            []float32         `json:"embedding,omitempty"`   // for retrieval
    AccessCount          int               `json:"access_count"`          // how often retrieved
    LastAccessedAt       *time.Time        `json:"last_accessed_at"`      // for decay
    Metadata             map[string]string `json:"metadata,omitempty"`
}
```

### PostgreSQL Schema

```sql
CREATE TABLE memory_entries (
    id                     TEXT PRIMARY KEY,
    tenant_id              TEXT NOT NULL,
    fact                   TEXT NOT NULL,
    source_conversation_id TEXT REFERENCES responses(id) ON DELETE SET NULL,
    confidence             REAL NOT NULL DEFAULT 1.0,
    tags                   TEXT[] NOT NULL DEFAULT '{}',
    embedding              vector(768),            -- pgvector column
    access_count           INTEGER NOT NULL DEFAULT 0,
    last_accessed_at       TIMESTAMPTZ,
    metadata               JSONB NOT NULL DEFAULT '{}',
    created_at             TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at             TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE INDEX idx_memory_tenant ON memory_entries(tenant_id);
CREATE INDEX idx_memory_source ON memory_entries(source_conversation_id);
CREATE INDEX idx_memory_tags ON memory_entries USING GIN (tags);
CREATE INDEX idx_memory_embedding ON memory_entries USING ivfflat (embedding vector_cosine_ops)
    WITH (lists = 100);
```

### Memory Extraction Process

After a conversation completes (or at configurable intervals), antwort can extract key facts from the conversation. This is done by asking the LLM itself to identify memorable facts.

```
┌─────────────────┐    ┌──────────────────┐    ┌─────────────────┐
│  Conversation   │───>│  Memory          │───>│  Store in       │
│  completes      │    │  extraction LLM  │    │  memory_entries │
│                 │    │  call             │    │  table          │
└─────────────────┘    └──────────────────┘    └─────────────────┘
```

The extraction prompt asks the model to identify:
- User preferences and decisions
- Key facts about the user's project or environment
- Action items or commitments
- Technical choices and rationale

Example extraction prompt:

```
You are a memory extraction system. Given the following conversation,
extract the key facts, decisions, and preferences expressed by the user.
Return each as a JSON object with fields: fact, confidence (0-1), tags.

Only extract facts that would be useful in future conversations.
Do not extract trivial or ephemeral details.

Conversation:
{conversation_text}
```

### Memory Retrieval at Runtime

When a new conversation starts, relevant memories are retrieved and injected into the system prompt or as additional context items.

```go
// MemoryStore is the interface for storing and retrieving episodic memories.
type MemoryStore interface {
    // Save stores a new memory entry.
    Save(ctx context.Context, entry *MemoryEntry) error

    // Search finds memories relevant to the given query text.
    // Uses vector similarity on the embedded query.
    Search(ctx context.Context, tenantID string, query string, limit int) ([]MemoryEntry, error)

    // SearchByTags finds memories matching specific tags.
    SearchByTags(ctx context.Context, tenantID string, tags []string, limit int) ([]MemoryEntry, error)

    // IncrementAccess updates the access count and timestamp when a memory is used.
    IncrementAccess(ctx context.Context, id string) error

    // Delete removes a memory entry.
    Delete(ctx context.Context, id string) error

    // Consolidate merges related memories into summaries.
    Consolidate(ctx context.Context, tenantID string) error
}
```

Retrieval happens early in the request flow, before the first provider call:

```
1. New request arrives with input text
2. Engine extracts the user's message text
3. Embeds the message text
4. Searches memory_entries by vector similarity (top 5-10 results)
5. Prepends relevant memories to the system prompt:
   "Previous context: The user prefers Python for scripting.
    The user's project uses PostgreSQL 16 with pgvector.
    ..."
6. Proceeds with normal inference
```

### Memory Consolidation

Over time, a user accumulates many memory entries. Consolidation merges related entries:

```
Before consolidation:
  mem_001: "User's project is called antwort"
  mem_002: "antwort is written in Go"
  mem_003: "antwort uses PostgreSQL for storage"
  mem_004: "antwort runs on Kubernetes"

After consolidation:
  mem_005: "User's project 'antwort' is a Go application using PostgreSQL,
            deployed on Kubernetes"
  (mem_001 through mem_004 are marked as consolidated, pointing to mem_005)
```

Consolidation can run as a periodic background job (configurable interval, e.g., daily) or be triggered manually via an API call.

### Memory Decay

Old, unused memories can be deprioritized. The retrieval scoring formula combines vector similarity with recency and access frequency:

```
score = similarity * (1.0 - decay_factor * days_since_last_access)
```

Memories that haven't been accessed in a configurable period (default: 90 days) can be automatically archived or deleted. This prevents the memory store from growing unboundedly.

### Privacy: Tenant Isolation

Memories are strictly tenant-scoped. The `tenant_id` column ensures that `memory_entries` for Tenant A are never visible to Tenant B. This follows the same pattern as `responses` table tenant isolation (Spec 005).

```go
func (s *PostgresMemoryStore) Search(ctx context.Context, tenantID string, query string, limit int) ([]MemoryEntry, error) {
    embedding, err := s.embedder.Embed(ctx, query)
    if err != nil {
        return nil, fmt.Errorf("embedding query: %w", err)
    }

    rows, err := s.pool.Query(ctx, `
        SELECT id, fact, confidence, tags, access_count, created_at
        FROM memory_entries
        WHERE tenant_id = $1
        ORDER BY embedding <=> $2
        LIMIT $3
    `, tenantID, pgvector.NewVector(embedding), limit)
    // ...
}
```

## Conversation Summarization

### The Problem

Long conversations produce long response chains. When `loadConversationHistory` in `pkg/engine/history.go` walks the chain, it collects every input and output item from every response. For a 50-turn conversation, this can easily exceed the LLM's context window.

Current behavior: the full chain is loaded without any truncation. The `Truncation` field on the request is echoed in the response but not yet enforced.

### Sliding Window + Summary

The solution is a sliding window with summarization:

```
Full conversation: [Turn 1] [Turn 2] [Turn 3] ... [Turn 48] [Turn 49] [Turn 50]

What gets sent to the LLM:
┌─────────────────────────┐ ┌───────────────────────────────────────┐
│  Summary of Turns 1-40  │ │  Full text of Turns 41-50            │
│  (condensed to ~500     │ │  (verbatim, preserving all detail)   │
│   tokens)               │ │                                       │
└─────────────────────────┘ └───────────────────────────────────────┘
```

The window size K (number of recent turns to keep verbatim) is configurable. Default: 10 turns.

### When Summarization Runs

Two strategies, configurable:

**Lazy (default)**: Summarization happens when the next request arrives and `loadConversationHistory` detects that the chain exceeds a token threshold. The summary is computed on-the-fly, then cached in the response metadata for future requests.

**Eager**: Summarization runs immediately after each response is saved, if the chain length exceeds the threshold. This adds latency to every response but ensures the summary is always ready.

```go
// SummarizationConfig controls when and how conversation history is summarized.
type SummarizationConfig struct {
    // Enabled turns on automatic summarization. Default: false.
    Enabled bool `json:"enabled" env:"ANTWORT_SUMMARIZATION_ENABLED"`

    // Strategy: "lazy" (on-demand) or "eager" (after each response).
    Strategy string `json:"strategy" env:"ANTWORT_SUMMARIZATION_STRATEGY"`

    // TokenThreshold: summarize when history exceeds this token count.
    // Default: 8000.
    TokenThreshold int `json:"token_threshold" env:"ANTWORT_SUMMARIZATION_TOKEN_THRESHOLD"`

    // WindowSize: number of recent turns to keep verbatim. Default: 10.
    WindowSize int `json:"window_size" env:"ANTWORT_SUMMARIZATION_WINDOW_SIZE"`

    // SummaryMaxTokens: target token count for the summary. Default: 500.
    SummaryMaxTokens int `json:"summary_max_tokens" env:"ANTWORT_SUMMARIZATION_MAX_TOKENS"`
}
```

### Where Summaries Are Stored

Summaries are stored in the `extensions` JSONB column of the `responses` table. This reuses existing infrastructure without schema changes:

```json
{
  "extensions": {
    "conversation_summary": {
      "summary": "The user asked about Kubernetes deployment strategies. We discussed kustomize overlays for dev/staging/prod environments, decided against Helm for this project, and configured resource limits...",
      "summarized_up_to": "resp_earlier_id",
      "token_count": 487,
      "created_at": 1709155200
    }
  }
}
```

When `loadConversationHistory` finds a summary in the chain, it uses the summary text instead of walking further back:

```go
func loadConversationHistory(ctx context.Context, store transport.ResponseStore, responseID string) ([]provider.ProviderMessage, error) {
    // Walk the chain as before...
    for currentID != "" {
        resp, err := store.GetResponseForChain(ctx, currentID)
        // ...

        // Check for a conversation summary in this response's extensions.
        if summary, ok := extractSummary(resp.Extensions); ok {
            // Insert summary as the first message, stop walking further.
            messages = prependSummary(summary, messages)
            break
        }

        chain = append(chain, resp)
        // continue walking...
    }
}
```

### Summarization Implementation

The summary is generated by making an additional LLM call with a summarization prompt:

```go
func (e *Engine) summarizeHistory(ctx context.Context, messages []provider.ProviderMessage) (string, error) {
    summaryReq := &provider.ProviderRequest{
        Model: e.cfg.SummarizationModel, // can be a smaller, faster model
        Messages: []provider.ProviderMessage{
            {
                Role:    "system",
                Content: "Summarize the following conversation concisely. Preserve key facts, decisions, and context that would be needed to continue the conversation. Do not include greetings or filler.",
            },
            {
                Role:    "user",
                Content: formatMessagesForSummary(messages),
            },
        },
        MaxTokens: e.cfg.Summarization.SummaryMaxTokens,
    }

    resp, err := e.provider.Complete(ctx, summaryReq)
    if err != nil {
        return "", fmt.Errorf("summarization failed: %w", err)
    }

    return extractTextFromResponse(resp), nil
}
```

The summarization model can be different from the main inference model. A smaller, faster model (e.g., a 7B parameter model) works well for summarization and keeps latency low.

## The file_search Tool Integration

### How it Maps to the Responses API

The `file_search` tool is declared in the request's `tools` array:

```json
{
  "tools": [
    {
      "type": "file_search",
      "vector_store_ids": ["vs_abc123", "vs_def456"]
    }
  ]
}
```

The engine resolves `vector_store_ids` from:
1. The request itself (explicit)
2. The agent profile, if `agent_id` is used (implicit)

### Integration with the Agentic Loop

`file_search` follows the same flow as any other server-executed tool in the agentic loop (Spec 004):

```
Engine executors (from Brainstorm 12):
├── FunctionRegistry (built-in tools)
│   ├── WebSearchProvider       (brainstorm 13)
│   ├── FileSearchProvider      (brainstorm 14, this document)
│   └── CodeInterpreterProvider (future, via sandbox)
├── MCPExecutor (external MCP servers)
└── (function tools -> requires_action, returned to client)
```

The `FileSearchProvider` implements the `FunctionProvider` interface from Brainstorm 12:

```go
type FileSearchProvider struct {
    vectorStore VectorStore
    embedder    Embedder
    config      FileSearchConfig
}

func (p *FileSearchProvider) Name() string { return "file_search" }

func (p *FileSearchProvider) Tools() []api.ToolDefinition {
    return []api.ToolDefinition{
        {
            Type:        "file_search",
            Name:        "file_search",
            Description: "Search uploaded files for relevant information",
        },
    }
}

func (p *FileSearchProvider) CanExecute(name string) bool {
    return name == "file_search"
}

func (p *FileSearchProvider) Execute(ctx context.Context, call tools.ToolCall) (*tools.ToolResult, error) {
    // 1. Parse query from arguments
    // 2. Get vector_store_ids from the tool definition or context
    // 3. Embed the query
    // 4. Search the vector store(s)
    // 5. Format and return results
}

func (p *FileSearchProvider) Routes() []Route {
    // Returns the vector store management API routes
}
```

### Relevance Scoring and Configuration

```go
type FileSearchConfig struct {
    // MaxResults is the maximum number of chunks to return per search.
    // Default: 10.
    MaxResults int `json:"max_results" env:"ANTWORT_FILESEARCH_MAX_RESULTS"`

    // MinScore is the minimum cosine similarity score for a result to be included.
    // Default: 0.5.
    MinScore float64 `json:"min_score" env:"ANTWORT_FILESEARCH_MIN_SCORE"`

    // ContextTokenBudget is the maximum token count for all returned chunks combined.
    // Prevents flooding the context window with too much retrieved content.
    // Default: 4000.
    ContextTokenBudget int `json:"context_token_budget" env:"ANTWORT_FILESEARCH_CONTEXT_BUDGET"`
}
```

## Storage Architecture

### pgvector as Default Vector Backend

pgvector extends the existing PostgreSQL database (from Spec 005) with vector operations. This means no new infrastructure for basic deployments.

```sql
-- Enable the extension (requires PostgreSQL with pgvector installed)
CREATE EXTENSION IF NOT EXISTS vector;

-- Vector store metadata
CREATE TABLE vector_stores (
    id          TEXT PRIMARY KEY,          -- "vs_abc123"
    tenant_id   TEXT NOT NULL,
    name        TEXT NOT NULL,
    status      TEXT NOT NULL DEFAULT 'completed',
    metadata    JSONB NOT NULL DEFAULT '{}',
    created_at  TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    expires_at  TIMESTAMPTZ,
    deleted_at  TIMESTAMPTZ
);

CREATE INDEX idx_vector_stores_tenant ON vector_stores(tenant_id);

-- File metadata
CREATE TABLE vector_store_files (
    id               TEXT PRIMARY KEY,     -- "vsf_file789"
    vector_store_id  TEXT NOT NULL REFERENCES vector_stores(id) ON DELETE CASCADE,
    tenant_id        TEXT NOT NULL,
    filename         TEXT NOT NULL,
    bytes            BIGINT NOT NULL,
    status           TEXT NOT NULL DEFAULT 'in_progress',
    chunk_count      INTEGER NOT NULL DEFAULT 0,
    chunking_config  JSONB NOT NULL DEFAULT '{}',
    error_message    TEXT,
    created_at       TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    completed_at     TIMESTAMPTZ
);

CREATE INDEX idx_vsf_store ON vector_store_files(vector_store_id);
CREATE INDEX idx_vsf_tenant ON vector_store_files(tenant_id);

-- Document chunks with vectors
CREATE TABLE vector_chunks (
    id               TEXT PRIMARY KEY,     -- "chunk_xyz"
    file_id          TEXT NOT NULL REFERENCES vector_store_files(id) ON DELETE CASCADE,
    vector_store_id  TEXT NOT NULL REFERENCES vector_stores(id) ON DELETE CASCADE,
    tenant_id        TEXT NOT NULL,
    content          TEXT NOT NULL,
    chunk_index      INTEGER NOT NULL,
    token_count      INTEGER NOT NULL,
    embedding        vector(768),          -- dimension depends on embedding model
    created_at       TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE INDEX idx_chunks_file ON vector_chunks(file_id);
CREATE INDEX idx_chunks_store ON vector_chunks(vector_store_id);
CREATE INDEX idx_chunks_tenant ON vector_chunks(tenant_id);

-- HNSW index for fast approximate nearest neighbor search
-- HNSW is preferred over IVFFlat for its better recall at similar speed
CREATE INDEX idx_chunks_embedding ON vector_chunks
    USING hnsw (embedding vector_cosine_ops)
    WITH (m = 16, ef_construction = 200);
```

### VectorStore Interface

```go
// VectorStore abstracts the vector storage backend.
// Default implementation uses pgvector. Alternative implementations
// can use Qdrant, Milvus, Weaviate, or other vector databases.
type VectorStore interface {
    // CreateStore creates a new vector store.
    CreateStore(ctx context.Context, store *VectorStoreMetadata) error

    // GetStore retrieves vector store metadata.
    GetStore(ctx context.Context, id string) (*VectorStoreMetadata, error)

    // ListStores lists all vector stores for a tenant.
    ListStores(ctx context.Context, tenantID string) ([]VectorStoreMetadata, error)

    // DeleteStore removes a vector store and all its data.
    DeleteStore(ctx context.Context, id string) error

    // IndexFile stores file metadata and its chunks.
    IndexFile(ctx context.Context, file *VectorStoreFile, chunks []Chunk) error

    // GetFile retrieves file metadata.
    GetFile(ctx context.Context, fileID string) (*VectorStoreFile, error)

    // ListFiles lists files in a vector store.
    ListFiles(ctx context.Context, vectorStoreID string) ([]VectorStoreFile, error)

    // DeleteFile removes a file and its chunks.
    DeleteFile(ctx context.Context, fileID string) error

    // Search performs vector similarity search across a set of vector stores.
    Search(ctx context.Context, storeIDs []string, queryEmbedding []float32, limit int) ([]SearchResult, error)

    // HealthCheck verifies the vector store connection.
    HealthCheck(ctx context.Context) error
}
```

### Embedding Generation

The `Embedder` interface abstracts embedding model access:

```go
// Embedder generates vector embeddings from text.
type Embedder interface {
    // Embed generates an embedding vector for the given text.
    Embed(ctx context.Context, text string) ([]float32, error)

    // EmbedBatch generates embedding vectors for multiple texts.
    // More efficient than calling Embed() in a loop.
    EmbedBatch(ctx context.Context, texts []string) ([][]float32, error)

    // Dimension returns the embedding vector dimension.
    Dimension() int
}
```

Embedding generation options:

```
┌───────────────────────────────────────────────────────┐
│                Embedder Interface                      │
│                                                        │
│  ┌─────────────────┐  ┌───────────────────────┐       │
│  │ Provider         │  │ Dedicated endpoint    │       │
│  │ (vLLM/LiteLLM   │  │ (separate embedding   │       │
│  │  /v1/embeddings) │  │  service URL)         │       │
│  └─────────────────┘  └───────────────────────┘       │
│                                                        │
│  ┌─────────────────┐                                   │
│  │ Sandbox Pod      │  Reuse sandbox infrastructure    │
│  │ (run embedding   │  for embedding (Brainstorm 11)   │
│  │  model in pod)   │                                  │
│  └─────────────────┘                                   │
└───────────────────────────────────────────────────────┘
```

The sandbox Pod option is interesting: if the sandbox infrastructure (Brainstorm 11) is already deployed, embedding models can run inside sandbox Pods using a `SandboxTemplate` configured for embedding. This avoids deploying a separate embedding service.

### Configuration

```yaml
builtins:
  file_search:
    enabled: true
    vector_backend: pgvector          # or "qdrant"
    embedding:
      url: http://llm-predictor:8080/v1/embeddings
      model: ""                        # use default model at the endpoint
      dimension: 768                   # must match the model's output dimension
      batch_size: 32                   # max texts per batch embed call
    chunking:
      strategy: fixed                  # "fixed", "recursive", or "semantic"
      chunk_size: 512                  # tokens per chunk
      chunk_overlap: 50                # overlap between consecutive chunks
    search:
      max_results: 10
      min_score: 0.5
      context_token_budget: 4000

    pgvector:
      dsn_file: /run/secrets/postgres/dsn   # reuse from storage (Spec 005)

    qdrant:
      url: http://qdrant:6333
      collection_prefix: antwort_

memory:
  enabled: false                       # episodic memory, opt-in
  extraction:
    model: ""                          # use default model, or specify a smaller one
    strategy: lazy                     # "lazy" or "eager"
  retrieval:
    max_results: 10
    min_score: 0.6
  consolidation:
    interval: 24h                      # how often to run consolidation
  decay:
    enabled: true
    half_life_days: 90                 # memories lose priority over time

summarization:
  enabled: false
  strategy: lazy
  token_threshold: 8000
  window_size: 10
  summary_max_tokens: 500
  model: ""                            # use default model, or specify a smaller one
```

## Integration with Agent Profiles

When agent profiles are implemented, they can reference vector stores:

```json
{
  "id": "agent_support_bot",
  "name": "Support Bot",
  "instructions": "You are a helpful support agent. Use the knowledge base to answer questions.",
  "tools": [
    {
      "type": "file_search",
      "vector_store_ids": ["vs_product_docs", "vs_faq"]
    }
  ],
  "memory": {
    "episodic": true,
    "summarization": true
  }
}
```

When a request uses `agent_id`, the agent's vector stores are automatically available to `file_search` without the client needing to specify them in every request.

Multiple agents can share the same vector store (read-only access). For example, all support agents might share a "product documentation" vector store, while each has their own episodic memories.

```
┌──────────────────┐     ┌──────────────────┐
│  Support Agent   │     │  Sales Agent     │
│  (agent_support) │     │  (agent_sales)   │
└────────┬─────────┘     └────────┬─────────┘
         │                        │
         ├── vs_product_docs ─────┤  (shared, read-only)
         │                        │
         ├── vs_support_kb        └── vs_sales_playbook
         │   (support only)           (sales only)
         │
         └── episodic memories
             (per agent, per tenant)
```

## Security Considerations

### Tenant Isolation

All data structures (vector stores, files, chunks, memory entries) include a `tenant_id` column. Every query filters by tenant. This follows the same pattern as the `responses` table.

```go
// Every vector store query includes tenant filtering
func (s *PgVectorStore) Search(ctx context.Context, storeIDs []string, embedding []float32, limit int) ([]SearchResult, error) {
    tenantID := storage.GetTenant(ctx)
    rows, err := s.pool.Query(ctx, `
        SELECT c.content, c.file_id, f.filename, c.chunk_index,
               1 - (c.embedding <=> $1) AS score
        FROM vector_chunks c
        JOIN vector_store_files f ON c.file_id = f.id
        WHERE c.vector_store_id = ANY($2)
          AND c.tenant_id = $3
        ORDER BY c.embedding <=> $1
        LIMIT $4
    `, pgvector.NewVector(embedding), storeIDs, tenantID, limit)
    // ...
}
```

### Document Access Control

| Operation | Required Permission |
|-----------|-------------------|
| Create vector store | Authenticated user (any tenant member) |
| Upload document | Vector store owner or write access |
| Delete document | Vector store owner or write access |
| Search (via file_search tool) | Any tenant member with access to the store |
| Delete vector store | Vector store owner only |

Access control is enforced at the management API layer. The `file_search` tool at runtime only searches stores the agent is configured to use, and the tenant filter prevents cross-tenant access.

### Embedding Model Access

The embedding model should ideally use the same provider infrastructure as inference. If vLLM or LiteLLM exposes a `/v1/embeddings` endpoint, use it. This keeps the configuration simple and avoids deploying a separate service.

If a separate embedding model is needed (for example, a specialized model like `nomic-embed-text-v1.5`), it can run as:
- A separate vLLM deployment
- A sidecar container
- A sandbox Pod (reusing sandbox infrastructure)

### Data Retention

Per-tenant retention policies control how long data is kept:

```yaml
# Per-tenant configuration (via tenant profile or admin API)
retention:
  vector_stores:
    max_storage_bytes: 10737418240   # 10 GB per tenant
    max_files_per_store: 1000
    max_file_size_bytes: 52428800    # 50 MB per file
  memory:
    max_entries: 10000               # per tenant
    auto_archive_days: 180           # archive after 180 days of no access
```

## Architecture Overview

```
┌─────────────────────────────────────────────────────────────────────┐
│                        Antwort Server                                │
│                                                                     │
│  ┌──────────────┐  ┌──────────────────┐  ┌───────────────────────┐ │
│  │  Transport    │  │  Engine           │  │  Function Registry   │ │
│  │  (HTTP/SSE)   │  │  (agentic loop)  │  │                      │ │
│  └──────┬───────┘  └────────┬─────────┘  │  ┌─────────────────┐ │ │
│         │                   │             │  │ FileSearch       │ │ │
│         │                   │             │  │ Provider         │ │ │
│         │                   │             │  └────────┬────────┘ │ │
│         │                   │             └───────────┼──────────┘ │
│         │                   │                         │            │
│  ┌──────┴───────────────────┴─────────────────────────┴──────────┐ │
│  │                     Storage Layer                              │ │
│  │                                                                │ │
│  │  ┌────────────┐  ┌──────────────┐  ┌───────────────────────┐  │ │
│  │  │ responses  │  │ vector_stores │  │ memory_entries       │  │ │
│  │  │ table      │  │ vector_chunks │  │ table                │  │ │
│  │  │ (Spec 005) │  │ tables        │  │ (episodic memory)    │  │ │
│  │  └────────────┘  └──────────────┘  └───────────────────────┘  │ │
│  │                                                                │ │
│  │                     PostgreSQL + pgvector                      │ │
│  └────────────────────────────────────────────────────────────────┘ │
│                                                                     │
│  ┌──────────────────────────────┐                                   │
│  │  Embedder                    │                                   │
│  │  (calls /v1/embeddings or   │                                   │
│  │   sandbox pod or dedicated  │                                   │
│  │   embedding service)        │                                   │
│  └──────────────────────────────┘                                   │
│                                                                     │
│  ┌──────────────────────────────┐                                   │
│  │  Summarizer                  │                                   │
│  │  (calls provider.Complete   │                                   │
│  │   with summarization prompt)│                                   │
│  └──────────────────────────────┘                                   │
└─────────────────────────────────────────────────────────────────────┘
```

### Data Flow: Complete Request with Memory and RAG

```
1. Client sends POST /v1/responses with file_search tool

2. Engine receives request
   ├── Extract user's message text
   ├── Retrieve episodic memories (if enabled)
   │   ├── Embed user message
   │   ├── Search memory_entries by vector similarity
   │   └── Prepend relevant memories to system prompt
   ├── Load conversation history (if previous_response_id set)
   │   ├── Walk response chain
   │   ├── If conversation summary exists, use it instead of full history
   │   └── Apply sliding window if needed
   └── Build provider request

3. Provider inference (turn 1)
   └── Model decides to call file_search("auth middleware design")

4. Agentic loop catches the tool call
   ├── FunctionRegistry routes to FileSearchProvider
   ├── FileSearchProvider.Execute():
   │   ├── Embed query: "auth middleware design"
   │   ├── Search vector_chunks WHERE tenant_id = $1 AND vector_store_id IN (...)
   │   ├── Rank by cosine similarity
   │   └── Return top-K chunks as tool result
   └── Feed result back to provider

5. Provider inference (turn 2)
   └── Model generates final answer using retrieved chunks

6. Response returned to client
   ├── Save response to storage
   ├── If summarization enabled and chain is long: generate summary
   └── If memory enabled: extract key facts asynchronously
```

## Open Questions

1. **Embedding model per vector store?**
   Should each vector store be able to use a different embedding model? This adds complexity (vectors from different models can't be compared) but enables specialized embeddings for different content types. Recommendation: start with a single global embedding model, add per-store override later.

2. **Document updates?**
   When a document is updated, should antwort re-embed the changed document automatically? Options:
   - Delete and re-upload (simple, client-driven)
   - Detect changes and re-embed only modified chunks (complex, server-driven)
   Recommendation: start with delete-and-reupload. Add incremental re-embedding later.

3. **Episodic memory: opt-in or opt-out?**
   Should memory extraction be enabled by default? Privacy-conscious users might not want their conversations analyzed for facts. Recommendation: opt-in (disabled by default), enabled per agent profile or per tenant.

4. **Sandbox access to vector stores?**
   Can code running in a sandbox Pod query vector stores directly? This would require exposing a vector search API inside the sandbox network. Recommendation: not initially. Sandbox code should call `file_search` through the agent loop, not access vectors directly.

5. **Embedding dimension consistency?**
   Different embedding models produce vectors of different dimensions (384, 768, 1024, 1536). The pgvector column type has a fixed dimension. If a user switches embedding models, existing vectors become incompatible. Options:
   - Store the dimension per vector store (each store is locked to one model)
   - Re-embed everything on model change
   Recommendation: dimension is fixed per vector store, set at creation time.

6. **Cross-tenant knowledge sharing?**
   Some deployments may want shared knowledge bases (e.g., company-wide documentation). How should this interact with tenant isolation? Options:
   - Special "shared" tenant ID that all tenants can read from
   - Vector stores with explicit sharing permissions (like file sharing)
   Recommendation: support a "shared" tenant scope for read-only global knowledge bases.

7. **Summarization model selection?**
   Should the summarization model be the same as the inference model, or a dedicated smaller model? A 7B model is fast and cheap for summarization, but adds deployment complexity.
   Recommendation: use the inference model by default, allow override via config.

## Deliverables

- [ ] `pkg/memory/memory.go` - MemoryEntry type and MemoryStore interface
- [ ] `pkg/memory/postgres/` - PostgreSQL implementation of MemoryStore
- [ ] `pkg/memory/extraction.go` - Memory extraction from conversations
- [ ] `pkg/memory/consolidation.go` - Memory consolidation logic
- [ ] `pkg/tools/builtins/filesearch/provider.go` - FileSearchProvider (FunctionProvider)
- [ ] `pkg/tools/builtins/filesearch/vectorstore.go` - VectorStore interface
- [ ] `pkg/tools/builtins/filesearch/pgvector/` - pgvector VectorStore implementation
- [ ] `pkg/tools/builtins/filesearch/embedder.go` - Embedder interface
- [ ] `pkg/tools/builtins/filesearch/embedder/openai/` - OpenAI-compatible embedder
- [ ] `pkg/tools/builtins/filesearch/chunker.go` - Document chunking strategies
- [ ] `pkg/tools/builtins/filesearch/parser.go` - File parsing (text, PDF, DOCX)
- [ ] `pkg/tools/builtins/filesearch/api.go` - Vector store management API routes
- [ ] `pkg/engine/summarize.go` - Conversation summarization
- [ ] `pkg/engine/history.go` - Extended with summary-aware chain walking
- [ ] `pkg/storage/postgres/migrations/XXX_create_vector_tables.sql`
- [ ] `pkg/storage/postgres/migrations/XXX_create_memory_table.sql`
- [ ] Configuration structs and validation
- [ ] Tests for all components
