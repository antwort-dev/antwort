= Quickstart 04: MCP Tools (Agentic Tool Calling)
:description: Deploy antwort with an MCP test server to demonstrate agentic tool calling via Streamable HTTP transport.

Deploy antwort with an MCP test server to demonstrate agentic tool calling.
The MCP server provides `get_time` and `echo` tools via Streamable HTTP transport, allowing the LLM to call external tools during response generation.

*Time to deploy*: 5 minutes (after LLM backend is running)

== Prerequisites

* xref:index.adoc[Shared LLM Backend] deployed and running
* `kubectl` or `oc` CLI configured

== Deploy

[source,bash]
----
# Create namespace
kubectl create namespace antwort

# Deploy antwort + MCP test server
kubectl apply -k quickstarts/04-mcp-tools/base/ -n antwort

# Wait for the MCP test server to be ready
kubectl rollout status deployment/mcp-test-server -n antwort --timeout=60s

# Wait for antwort to be ready
kubectl rollout status deployment/antwort -n antwort --timeout=60s
----

=== OpenShift / ROSA

For external access via Route:

[source,bash]
----
# Apply with OpenShift overlay
kubectl apply -k quickstarts/04-mcp-tools/openshift/ -n antwort

# Get the route URL
ROUTE=$(kubectl get route antwort -n antwort -o jsonpath='{.spec.host}')
echo "Antwort URL: https://$ROUTE"
----

== Test

=== Setup Port-Forward

[source,bash]
----
# Using port-forward (vanilla Kubernetes)
kubectl port-forward -n antwort svc/antwort 8080:8080 &

# Or using the Route URL (OpenShift)
# export URL=https://$ROUTE

export URL=http://localhost:8080
----

=== Test the get_time Tool

Ask the LLM a question that requires knowing the current time.
The agentic loop will detect that the `get_time` tool is needed, call it via MCP, and include the result in the response.

[source,bash]
----
curl -s -X POST "$URL/v1/responses" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "/mnt/models",
    "input": [
      {
        "type": "message",
        "role": "user",
        "content": [{"type": "input_text", "text": "What time is it? Use the get_time tool."}]
      }
    ]
  }' | jq '{status: .status, output: [.output[] | {type: .type, content: (if .type == "message" then .content[0].text elif .type == "mcp_call" then {name: .name, server: .server_label} elif .type == "mcp_call_output" then {output: .output} else . end)}]}'
----

Expected output should include both the tool call and the final response:

[source,json]
----
{
  "status": "completed",
  "output": [
    {
      "type": "mcp_call",
      "content": { "name": "get_time", "server": "test-tools" }
    },
    {
      "type": "mcp_call_output",
      "content": { "output": "..." }
    },
    {
      "type": "message",
      "content": "The current time is ..."
    }
  ]
}
----

=== Test the echo Tool

The echo tool returns whatever text you send to it, useful for verifying the tool-calling pipeline:

[source,bash]
----
curl -s -X POST "$URL/v1/responses" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "/mnt/models",
    "input": [
      {
        "type": "message",
        "role": "user",
        "content": [{"type": "input_text", "text": "Use the echo tool to echo back the phrase: Hello from MCP!"}]
      }
    ]
  }' | jq '{status: .status, output: [.output[] | {type: .type, content: (if .type == "message" then .content[0].text elif .type == "mcp_call" then {name: .name, server: .server_label, arguments: .arguments} elif .type == "mcp_call_output" then {output: .output} else . end)}]}'
----

=== Streaming with Tools

Tool calls also work with streaming enabled:

[source,bash]
----
curl -s -N -X POST "$URL/v1/responses" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "/mnt/models",
    "stream": true,
    "input": [
      {
        "type": "message",
        "role": "user",
        "content": [{"type": "input_text", "text": "What time is it right now? Use the get_time tool to find out."}]
      }
    ]
  }'
----

You should see SSE events including `response.mcp_call.started`, `response.mcp_call_output.done`, and the text output delta events.

=== Verify Available Tools

Check that antwort discovered the tools from the MCP server by looking at the metrics:

[source,bash]
----
curl -s "$URL/metrics" | grep mcp
----

=== Structured Output

Request a response with a JSON schema to get structured output:

[source,bash]
----
curl -s -X POST "$URL/v1/responses" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "/mnt/models",
    "input": [
      {
        "type": "message",
        "role": "user",
        "content": [{"type": "input_text", "text": "List 3 programming languages with their year of creation"}]
      }
    ],
    "text": {
      "format": {
        "type": "json_schema",
        "name": "languages",
        "schema": {
          "type": "object",
          "properties": {
            "languages": {
              "type": "array",
              "items": {
                "type": "object",
                "properties": {
                  "name": {"type": "string"},
                  "year": {"type": "integer"}
                },
                "required": ["name", "year"]
              }
            }
          },
          "required": ["languages"]
        }
      }
    }
  }' | jq '.output[] | select(.type == "message") | .content[0].text' -r | jq .
----

The `text.format` field constrains the model to produce valid JSON matching the schema.
Expected output:

[source,json]
----
{
  "languages": [
    {"name": "Python", "year": 1991},
    {"name": "JavaScript", "year": 1995},
    {"name": "Go", "year": 2009}
  ]
}
----

=== Reasoning

Request a response with reasoning to see the model's thought process:

[source,bash]
----
curl -s -X POST "$URL/v1/responses" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "/mnt/models",
    "input": [
      {
        "type": "message",
        "role": "user",
        "content": [{"type": "input_text", "text": "What is 15% of 240?"}]
      }
    ],
    "reasoning": {"effort": "medium"}
  }' | jq '{
    output_types: [.output[].type],
    reasoning: [.output[] | select(.type == "reasoning") | .summary[0].text],
    answer: [.output[] | select(.type == "message") | .content[0].text]
  }'
----

Reasoning output depends on model support.
If the model does not support reasoning, the response will complete normally without reasoning items.

== What is Deployed

[cols="1,3",options="header"]
|===
| Component | Description

| antwort
| OpenResponses gateway with MCP configuration (1 Pod)

| mcp-test-server
| MCP test server providing `get_time` and `echo` tools (1 Pod)

| ConfigMap
| In-memory storage, MCP server connection config

| Services
| ClusterIP for both antwort (8080) and mcp-test-server (8080)

| Route (OpenShift)
| Edge TLS for external access to antwort
|===

== Configuration

The `config.yaml` in the ConfigMap connects to the MCP test server:

[source,yaml]
----
server:
  port: 8080

engine:
  provider: vllm
  backend_url: http://llm-predictor.llm-serving.svc.cluster.local:8080
  default_model: /mnt/models
  max_turns: 10

storage:
  type: memory
  max_size: 10000

auth:
  type: none

mcp:
  servers:
    - name: test-tools
      transport: streamable-http
      url: http://mcp-test-server:8080/mcp
----

The MCP test server image (`quay.io/rhuss/antwort:mcp-test`) provides two tools:

[cols="1,3",options="header"]
|===
| Tool | Description

| `get_time`
| Returns the current server time

| `echo`
| Echoes back the provided text
|===

== Next Steps

Ready for more?
Continue to xref:qs-05-code-interpreter.adoc[Quickstart 05: Code Interpreter] to add sandbox code execution.

== Cleanup

[source,bash]
----
kubectl delete namespace antwort
----
