= Python SDK
:description: Using Antwort with the OpenAI Python SDK.

This page walks through common use cases with the https://github.com/openai/openai-python[OpenAI Python SDK].

== Setup

Install the SDK:

[source,bash]
----
pip install openai
----

Create a client pointing at your Antwort instance:

[source,python]
----
from openai import OpenAI

client = OpenAI(
    base_url="https://antwort.example.com/v1",
    api_key="your-api-key",
)
----

TIP: Set `OPENAI_BASE_URL` and `OPENAI_API_KEY` as environment variables to avoid hardcoding credentials.

== Simple Text Response

[source,python]
----
response = client.responses.create(
    model="qwen/Qwen2.5-7B-Instruct",
    input="What is Kubernetes?",
)

print(response.output_text)
----

The `input` parameter accepts a plain string for simple prompts.
For structured input with roles, pass a list of input items (see xref:patterns.adoc[Common Patterns]).

== Streaming

Streaming delivers response tokens as they are generated:

[source,python]
----
stream = client.responses.create(
    model="qwen/Qwen2.5-7B-Instruct",
    input="Explain container networking in three sentences.",
    stream=True,
)

for event in stream:
    if event.type == "response.output_text.delta":
        print(event.delta, end="", flush=True)

print()  # newline after stream completes
----

The stream yields events following the SSE lifecycle.
The most common events are:

- `response.output_text.delta` for incremental text chunks
- `response.completed` when the response is finished

== Tool Calling

Antwort executes server-side tools (MCP tools, code interpreter) automatically.
The model decides when to call a tool, Antwort executes it, feeds the result back, and continues until a final text answer is produced.

From the client's perspective, you simply include the tool definitions in your request:

[source,python]
----
response = client.responses.create(
    model="qwen/Qwen2.5-7B-Instruct",
    input="What time is it in Berlin?",
    tools=[
        {
            "type": "function",
            "name": "get_time",
            "description": "Get the current time in a timezone",
            "parameters": {
                "type": "object",
                "properties": {
                    "timezone": {
                        "type": "string",
                        "description": "IANA timezone name",
                    }
                },
                "required": ["timezone"],
            },
        }
    ],
)

print(response.output_text)
----

NOTE: The tools must be registered on the Antwort server (via MCP or built-in tool configuration).
The `tools` array in the request tells the model which tools are available.
Antwort handles execution transparently.

== Conversation Chaining

Use `previous_response_id` to build multi-turn conversations:

[source,python]
----
# First turn
response = client.responses.create(
    model="qwen/Qwen2.5-7B-Instruct",
    input="My name is Alice.",
)

# Second turn (remembers context)
follow_up = client.responses.create(
    model="qwen/Qwen2.5-7B-Instruct",
    input="What is my name?",
    previous_response_id=response.id,
)

print(follow_up.output_text)  # "Your name is Alice."
----

Antwort reconstructs the full conversation history from stored responses.
This requires a storage backend (PostgreSQL or in-memory).

== Structured Output

Constrain the model output to a JSON schema:

[source,python]
----
response = client.responses.create(
    model="qwen/Qwen2.5-7B-Instruct",
    input="List three European capitals.",
    text={
        "format": {
            "type": "json_schema",
            "name": "capitals",
            "schema": {
                "type": "object",
                "properties": {
                    "capitals": {
                        "type": "array",
                        "items": {"type": "string"},
                    }
                },
                "required": ["capitals"],
            },
        }
    },
)

import json
data = json.loads(response.output_text)
print(data["capitals"])
----

== Listing Available Models

Query which models your Antwort instance serves:

[source,python]
----
models = client.models.list()
for model in models:
    print(model.id)
----
