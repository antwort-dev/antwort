= Monitoring
:navtitle: Monitoring

Antwort exposes Prometheus metrics on the `/metrics` endpoint, which is enabled by default.
This page documents every metric, explains how to collect them with Kubernetes-native tooling, and provides example alert rules.

== Metrics Endpoint

The metrics endpoint is controlled by the `observability` configuration section.

[source,yaml]
----
observability:
  metrics:
    enabled: true     # <1>
    path: /metrics    # <2>
----
<1> Metrics are enabled by default. Set to `false` to disable.
<2> The HTTP path where Prometheus metrics are served.

The `/metrics` endpoint is excluded from authentication middleware, so Prometheus can scrape it without credentials.

== Gateway Metrics

These metrics track request processing at the gateway layer.

[cols="3,1,2,3", options="header"]
|===
| Metric | Type | Labels | Description

| `antwort_requests_total`
| Counter
| `method`, `status`, `model`
| Total HTTP requests received by the gateway.

| `antwort_request_duration_seconds`
| Histogram
| `method`, `model`
| End-to-end request duration. Buckets are tuned for LLM inference latencies (0.1s to 120s).

| `antwort_streaming_connections_active`
| Gauge
| (none)
| Number of active SSE streaming connections at any given moment.

| `antwort_provider_requests_total`
| Counter
| `provider`, `model`, `status`
| Requests sent to backend LLM providers.

| `antwort_provider_latency_seconds`
| Histogram
| `provider`, `model`
| Backend provider response latency. Uses the same LLM-tuned buckets as request duration.

| `antwort_provider_tokens_total`
| Counter
| `provider`, `model`, `direction`
| Token usage by direction. The `direction` label is either `input` or `output`.

| `antwort_tool_executions_total`
| Counter
| `tool_name`, `status`
| Tool call executions by tool name and outcome.

| `antwort_ratelimit_rejected_total`
| Counter
| `tier`
| Requests rejected by the rate limiter, broken down by service tier.
|===

== Built-in Tool Provider Metrics

These metrics track the built-in function provider subsystem and its HTTP API routes.

[cols="3,1,2,3", options="header"]
|===
| Metric | Type | Labels | Description

| `antwort_builtin_tool_executions_total`
| Counter
| `provider`, `tool_name`, `status`
| Executions of built-in tools by provider, tool name, and outcome.

| `antwort_builtin_tool_duration_seconds`
| Histogram
| `provider`, `tool_name`
| Execution duration for built-in tools. Buckets range from 10ms to 60s.

| `antwort_builtin_api_requests_total`
| Counter
| `provider`, `method`, `path`, `status`
| HTTP API requests to built-in provider routes.

| `antwort_builtin_api_duration_seconds`
| Histogram
| `provider`, `method`, `path`
| Duration of built-in provider API requests.
|===

== GenAI Semantic Convention Metrics

Antwort emits metrics following the https://opentelemetry.io/docs/specs/semconv/gen-ai/[OpenTelemetry GenAI semantic conventions].
These metrics provide a vendor-neutral view of LLM usage and are compatible with observability platforms that understand the `gen_ai.*` namespace.

[cols="3,1,3", options="header"]
|===
| Metric | Type | Labels

| `gen_ai_client_token_usage`
| Histogram
| `gen_ai_operation_name`, `gen_ai_provider_name`, `gen_ai_token_type`, `gen_ai_request_model`, `gen_ai_response_model`

| `gen_ai_client_operation_duration_seconds`
| Histogram
| `gen_ai_operation_name`, `gen_ai_provider_name`, `gen_ai_request_model`, `gen_ai_response_model`, `error_type`

| `gen_ai_server_time_to_first_token_seconds`
| Histogram
| `gen_ai_operation_name`, `gen_ai_provider_name`, `gen_ai_request_model`

| `gen_ai_server_time_per_output_token_seconds`
| Histogram
| `gen_ai_operation_name`, `gen_ai_provider_name`, `gen_ai_request_model`
|===

The `gen_ai_token_type` label is either `input` or `output`.
Token usage histograms use power-of-4 buckets from 1 to 16384, while duration histograms use the same LLM-tuned buckets as the gateway metrics.

== Collecting Metrics with ServiceMonitor

If you are running the Prometheus Operator (or OpenShift monitoring), create a ServiceMonitor to scrape antwort automatically.

[source,yaml]
----
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: antwort
  labels:
    app.kubernetes.io/name: antwort
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: antwort
  endpoints:
    - port: http
      path: /metrics
      interval: 30s
----

On OpenShift, place the ServiceMonitor in the same namespace as the antwort Deployment so that user-workload monitoring picks it up.

== Grafana Dashboard

We recommend importing a dashboard that visualizes the key operational signals.
A suitable starting point organizes panels into three rows.

**Request overview:**

* Requests per second by status (`rate(antwort_requests_total[5m])`)
* Request duration p50/p95/p99 (`histogram_quantile(0.95, rate(antwort_request_duration_seconds_bucket[5m]))`)
* Active streaming connections (`antwort_streaming_connections_active`)

**Provider backend:**

* Provider requests per second by status
* Provider latency p95
* Token throughput by direction (`rate(antwort_provider_tokens_total[5m])`)

**Tools and rate limiting:**

* Tool execution rate by name and status
* Rate limit rejections by tier

The quickstart `02-production` includes a Grafana configuration with data source provisioning that you can use as a reference.

== Alert Rules

The following PrometheusRule examples detect common failure conditions.

=== High Error Rate

[source,yaml]
----
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: antwort-alerts
spec:
  groups:
    - name: antwort.rules
      rules:
        - alert: AntwortHighErrorRate
          expr: |
            sum(rate(antwort_requests_total{status=~"5.."}[5m]))
            /
            sum(rate(antwort_requests_total[5m]))
            > 0.05
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Antwort error rate exceeds 5%"
            description: >
              More than 5% of requests are returning 5xx status codes
              over the last 5 minutes.
----

=== High Latency

[source,yaml]
----
        - alert: AntwortHighLatency
          expr: |
            histogram_quantile(0.95,
              sum(rate(antwort_request_duration_seconds_bucket[5m])) by (le)
            ) > 30
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Antwort p95 latency exceeds 30 seconds"
            description: >
              The 95th percentile request duration has exceeded 30 seconds
              for more than 5 minutes. Check backend provider health.
----

=== Provider Backend Down

[source,yaml]
----
        - alert: AntwortProviderErrors
          expr: |
            sum(rate(antwort_provider_requests_total{status=~"5.."}[5m])) by (provider)
            > 0
          for: 2m
          labels:
            severity: critical
          annotations:
            summary: "Backend provider returning errors"
            description: >
              Provider {{ $labels.provider }} is returning 5xx errors.
              Check the LLM backend Pod status and logs.
----

=== Rate Limiting Active

[source,yaml]
----
        - alert: AntwortRateLimiting
          expr: |
            sum(rate(antwort_ratelimit_rejected_total[5m])) by (tier)
            > 1
          for: 5m
          labels:
            severity: info
          annotations:
            summary: "Rate limiting active for tier {{ $labels.tier }}"
            description: >
              Requests are being rejected for the {{ $labels.tier }} tier.
              Review rate limit configuration or consider increasing limits.
----
