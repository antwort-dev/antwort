# Feature Specification: Quickstart Updates

**Feature Branch**: `031-quickstart-updates`
**Created**: 2026-02-28
**Status**: Draft
**Input**: Add 05-code-interpreter and 06-responses-proxy quickstarts, refresh existing 01-04 READMEs with structured output and reasoning examples.

## User Scenarios & Testing *(mandatory)*

### User Story 1 - Code Interpreter Quickstart (Priority: P1)

A developer wants to deploy antwort with sandbox code execution so that the LLM can write and run Python code during response generation. They follow the quickstart to deploy a sandbox-server alongside antwort, send a prompt requesting a calculation, and observe the LLM generating code, executing it in the sandbox, and returning the result.

**Why this priority**: Code execution is a flagship agentic capability. It demonstrates the most visually impressive feature (LLM writing and running code) and exercises the full tool lifecycle pipeline.

**Independent Test**: Deploy the 05-code-interpreter quickstart from scratch (with shared LLM backend), send a math prompt, and verify the response contains a `code_interpreter_call` output item with execution results.

**Acceptance Scenarios**:

1. **Given** the shared LLM backend, sandbox-server, and antwort are deployed, **When** a user sends "Calculate the first 20 Fibonacci numbers using Python", **Then** the response contains a `code_interpreter_call` item with stdout showing the Fibonacci sequence and a final `message` summarizing the result.
2. **Given** the sandbox-server is running in Python mode, **When** a user sends a request requiring a package ("Use numpy to calculate the standard deviation of [1,2,3,4,5]"), **Then** the sandbox installs the package and returns correct output.
3. **Given** the OpenShift overlay is applied, **When** a user accesses antwort via the Route URL, **Then** code execution works identically through the external endpoint.

---

### User Story 2 - Responses API Proxy Quickstart (Priority: P2)

A developer wants to deploy antwort as a proxy/gateway in front of another antwort instance, creating a two-tier architecture. The frontend instance uses the "responses" provider to forward requests to the backend instance running the vllm provider. This demonstrates the middleware/gateway deployment pattern.

**Why this priority**: The Responses API provider (Spec 030) enables antwort-to-antwort chaining, a key architectural pattern for adding auth, rate limiting, or routing in front of inference backends. It's the newest provider and needs a reference deployment.

**Independent Test**: Deploy backend antwort (vllm provider), frontend antwort (responses provider pointing to backend), send a prompt to the frontend, and verify the response is generated by the backend LLM.

**Acceptance Scenarios**:

1. **Given** both backend and frontend antwort instances are deployed, **When** a user sends a text completion request to the frontend, **Then** the response is generated by the backend's LLM and returned through the frontend.
2. **Given** streaming is enabled, **When** a user sends a streaming request to the frontend, **Then** SSE events flow through both instances and the user receives incremental output.
3. **Given** the OpenShift overlay is applied, **When** a user accesses the frontend via its Route URL, **Then** the proxy chain works end-to-end through the external endpoint.

---

### User Story 3 - Refresh Existing Quickstart READMEs (Priority: P2)

A developer using any existing quickstart (01-04) wants to see examples of newer API features: structured output (json_schema response format) and reasoning content. The READMEs are updated with additional test sections demonstrating these capabilities.

**Why this priority**: Keeps existing quickstarts current with shipped features. Low effort, high documentation value. Equal priority to US2 since both improve the overall quickstart experience.

**Independent Test**: Follow any existing quickstart README, run the new structured output and reasoning test commands, and verify they produce the expected output format.

**Acceptance Scenarios**:

1. **Given** the 01-minimal quickstart is deployed, **When** a user runs the structured output curl example, **Then** the response contains a `text` output with valid JSON matching the requested schema.
2. **Given** any quickstart (01-04) is deployed, **When** a user runs the reasoning example, **Then** the response includes reasoning content items (if the model supports it) or completes normally (if not).
3. **Given** a developer reads a quickstart README, **When** they reach the "Next Steps" section, **Then** they see a reference to the code interpreter quickstart as the natural progression.

---

### Edge Cases

- What happens when the sandbox-server is unavailable? Antwort returns an error in the `code_interpreter_call` output item with a clear message.
- What happens when the upstream backend in the proxy chain is down? The frontend returns an appropriate error response.
- What happens when a model does not support reasoning? The reasoning example completes normally without reasoning items. The README notes this behavior.
- What happens when the LLM generates invalid code? The sandbox returns stderr output and a non-zero exit code, which is included in the response.

## Requirements *(mandatory)*

### Functional Requirements

- **FR-001**: The 05-code-interpreter quickstart MUST include a sandbox-server Deployment, Service, and ConfigMap as a sidecar component in the namespace.
- **FR-002**: The 05-code-interpreter quickstart MUST configure antwort with `code_interpreter.sandbox_url` pointing to the sandbox Service.
- **FR-003**: The 05-code-interpreter quickstart MUST include at least three test examples: basic computation, package installation, and data analysis.
- **FR-004**: The 05-code-interpreter quickstart MUST include an "Advanced: SandboxClaim" section explaining production-grade sandbox acquisition via the agent-sandbox operator (documentation only, not deployed in the quickstart).
- **FR-005**: The 06-responses-proxy quickstart MUST deploy two antwort instances: a backend (vllm provider) and a frontend (responses provider).
- **FR-006**: The 06-responses-proxy quickstart MUST include separate ConfigMaps, Deployments, and Services for backend and frontend instances.
- **FR-007**: The 06-responses-proxy quickstart MUST include test examples for both non-streaming and streaming requests through the proxy chain.
- **FR-008**: Both new quickstarts MUST include an OpenShift overlay with Route resources.
- **FR-009**: Both new quickstarts MUST follow the existing quickstart structure: base/ directory with kustomization.yaml, openshift/ overlay, and a README.md with deploy/test/cleanup sections.
- **FR-010**: Existing quickstarts (01-04) MUST be updated with a "Structured Output" test section showing json_schema response format usage.
- **FR-011**: Existing quickstarts (01-04) MUST be updated with a "Reasoning" test section showing reasoning content in responses.
- **FR-012**: Existing quickstarts (01-04) MUST be updated with a "Next Steps" section referencing the next quickstart in the progression.
- **FR-013**: The sandbox-server container image MUST be built from the existing `Containerfile.sandbox` in the project root.
- **FR-014**: All quickstart manifests MUST use image references consistent with existing quickstarts (configurable via kustomization.yaml image overrides).

### Key Entities

- **Quickstart**: A self-contained, deployable example consisting of Kubernetes manifests (base + optional overlays) and a README with step-by-step instructions.
- **Sandbox Server**: A lightweight HTTP service that executes code in isolated subprocesses, deployed as a separate pod in the 05-code-interpreter quickstart.
- **Proxy Chain**: A two-tier deployment where a frontend antwort instance forwards requests to a backend antwort instance using the Responses API provider.

## Success Criteria *(mandatory)*

### Measurable Outcomes

- **SC-001**: A developer can deploy the 05-code-interpreter quickstart and execute a code generation request within 10 minutes (after LLM backend is running).
- **SC-002**: A developer can deploy the 06-responses-proxy quickstart and complete a request through the proxy chain within 10 minutes (after LLM backend is running).
- **SC-003**: All curl test commands in quickstart READMEs (01-06) produce the documented expected output when run against a healthy deployment.
- **SC-004**: Each quickstart README is self-contained: a developer can follow it without referring to other documentation (except prerequisites).
- **SC-005**: The structured output test example in each refreshed quickstart returns valid JSON matching the specified schema in 100% of test runs.

## Assumptions

- The shared LLM backend (quickstart 00) uses a model capable of generating Python code (Qwen 2.5 7B Instruct, which is confirmed capable).
- Reasoning output depends on model support. The README will note that models without reasoning capability will skip reasoning items gracefully.
- The sandbox-server container image is available in a registry accessible to the cluster. The quickstart uses image overrides to allow pointing to any registry.
- The Responses API provider (Spec 030) is fully implemented and supports both streaming and non-streaming proxying.

## Dependencies

- Spec 024 (Sandbox Server): provides the sandbox-server binary and Containerfile.sandbox
- Spec 025 (Code Interpreter): provides the code_interpreter built-in tool
- Spec 030 (Responses API Provider): provides the "responses" provider type
- Spec 029 (Structured Output): provides text.format json_schema passthrough
- Spec 021 (Reasoning Streaming): provides reasoning content items
- Spec 015 (Quickstarts): the existing quickstart series this extends

## Out of Scope

- SandboxClaim-based deployment (documented as advanced reference only, not deployed)
- MCP-secured quickstart (blocked on OAuth token exchange)
- RAG quickstart (blocked on external RAG server)
- Building and pushing container images (quickstarts assume pre-built images)
- Multi-mode sandbox (Shell, Go, Node.js modes are documented but not exercised in the quickstart)
