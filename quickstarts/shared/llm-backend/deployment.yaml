apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-predictor
  labels:
    app: llm-predictor
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llm-predictor
  template:
    metadata:
      labels:
        app: llm-predictor
    spec:
      containers:
        - name: vllm
          image: docker.io/vllm/vllm-openai:v0.6.4
          args:
            - "--model"
            - "/mnt/models"
            - "--port"
            - "8080"
            - "--max-model-len"
            - "8192"
            - "--enable-auto-tool-choice"
            - "--tool-call-parser"
            - "hermes"
            - "--disable-frontend-multiprocessing"
          env:
            - name: HOME
              value: /tmp
            - name: HF_HOME
              value: /tmp/.cache
          ports:
            - containerPort: 8080
              name: http
          resources:
            limits:
              nvidia.com/gpu: "1"
            requests:
              nvidia.com/gpu: "1"
          volumeMounts:
            - name: model-storage
              mountPath: /mnt/models
            - name: shm
              mountPath: /dev/shm
            - name: tmp
              mountPath: /tmp
          readinessProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 30
            periodSeconds: 10
          livenessProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 60
            periodSeconds: 30
      volumes:
        - name: model-storage
          persistentVolumeClaim:
            claimName: llm-model-storage
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 12Gi
        - name: tmp
          emptyDir: {}
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
